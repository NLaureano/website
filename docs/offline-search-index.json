[{"body":" The documentation in this section explains core Fleet concepts. Pick one below to proceed.\n","categories":"","description":"Core concepts in Fleet","excerpt":"Core concepts in Fleet","ref":"/website/docs/concepts/","tags":"","title":"Concepts"},{"body":" Fleet documentation features a number of getting started tutorials to help you learn about Fleet with an environment of your preference. Pick one below to proceed.\nIf you are not sure about which one is the best option, for simplicity reasons, it is recommended that you start with the Getting started with Fleet using KinD clusters.\n","categories":"","description":"Getting started with Fleet","excerpt":"Getting started with Fleet","ref":"/website/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"This how-to guide discusses how to manage clusters in a fleet, specifically:\nhow to join a cluster into a fleet; and how to set a cluster to leave a fleet; and how to add labels to a member cluster Joining a cluster into a fleet A cluster can join in a fleet if:\nit runs a supported Kubernetes version; it is recommended that you use Kubernetes 1.24 or later versions, and it has network connectivity to the hub cluster of the fleet. For your convenience, Fleet provides a script that can automate the process of joining a cluster into a fleet. To use the script, run the commands below:\nNote\nTo run this script, make sure that you have already installed the following tools in your system:\nkubectl, the Kubernetes CLI helm, a Kubernetes package manager curl jq base64 # Replace the value of HUB_CLUSTER_CONTEXT with the name of the kubeconfig context you use for # accessing your hub cluster. export HUB_CLUSTER_CONTEXT=YOUR-HUB-CLUSTER-CONTEXT # Replace the value of HUB_CLUSTER_ADDRESS with the address of your hub cluster API server. export HUB_CLUSTER_ADDRESS=YOUR-HUB-CLUSTER-ADDRESS # Replace the value of MEMBER_CLUSTER with the name you would like to assign to the new member # cluster. # # Note that Fleet will recognize your cluster with this name once it joins. export MEMBER_CLUSTER=YOUR-MEMBER-CLUSTER # Replace the value of MEMBER_CLUSTER_CONTEXT with the name of the kubeconfig context you use # for accessing your member cluster. export MEMBER_CLUSTER_CONTEXT=YOUR-MEMBER-CLUSTER-CONTEXT # Clone the Fleet GitHub repository. git clone https://github.com/Azure/fleet.git # Run the script. chmod +x fleet/hack/membership/join.sh ./fleet/hack/membership/join.sh It may take a few minutes for the script to finish running. Once it is completed, verify that the cluster has joined successfully with the command below:\nkubectl config use-context $HUB_CLUSTER_CONTEXT kubectl get membercluster $MEMBER_CLUSTER If you see that the cluster is still in an unknown state, it might be that the member cluster is still connecting to the hub cluster. Should this state persist for a prolonged period, refer to the Troubleshooting Guide for more information.\nAlternatively, if you would like to find out the exact steps the script performs, or if you feel like fine-tuning some of the steps, you may join a cluster manually to your fleet with the instructions below:\nJoining a member cluster manually Make sure that you have installed kubectl, helm, curl, jq, and base64 in your system.\nCreate a Kubernetes service account in your hub cluster:\n# Replace the value of HUB_CLUSTER_CONTEXT with the name of the kubeconfig # context you use for accessing your hub cluster. export HUB_CLUSTER_CONTEXT=\"YOUR-HUB-CLUSTER-CONTEXT\" # Replace the value of MEMBER_CLUSTER with a name you would like to assign to the new # member cluster. # # Note that the value of MEMBER_CLUSTER will be used as the name the member cluster registers # with the hub cluster. export MEMBER_CLUSTER=\"YOUR-MEMBER-CLUSTER\" export SERVICE_ACCOUNT=\"$MEMBER_CLUSTER-hub-cluster-access\" kubectl config use-context $HUB_CLUSTER_CONTEXT # The service account can, in theory, be created in any namespace; for simplicity reasons, # here you will use the namespace reserved by Fleet installation, `fleet-system`. # # Note that if you choose a different value, commands in some steps below need to be # modified accordingly. kubectl create serviceaccount $SERVICE_ACCOUNT -n fleet-system Create a Kubernetes secret of the service account token type, which the member cluster will use to access the hub cluster.\nexport SERVICE_ACCOUNT_SECRET=\"$MEMBER_CLUSTER-hub-cluster-access-token\" cat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: $SERVICE_ACCOUNT_SECRET namespace: fleet-system annotations: kubernetes.io/service-account.name: $SERVICE_ACCOUNT type: kubernetes.io/service-account-token EOF After the secret is created successfully, extract the token from the secret:\nexport TOKEN=$(kubectl get secret $SERVICE_ACCOUNT_SECRET -n fleet-system -o jsonpath='{.data.token}' | base64 -d) Note\nKeep the token in a secure place; anyone with access to this token can access the hub cluster in the same way as the Fleet member cluster does.\nYou may have noticed that at this moment, no access control has been set on the service account; Fleet will set things up when the member cluster joins. The service account will be given the minimally viable set of permissions for the Fleet member cluster to connect to the hub cluster; its access will be restricted to one namespace, specifically reserved for the member cluster, as per security best practices.\nRegister the member cluster with the hub cluster; Fleet manages cluster membership using the MemberCluster API:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: cluster.kubernetes-fleet.io/v1beta1 kind: MemberCluster metadata: name: $MEMBER_CLUSTER spec: identity: name: $SERVICE_ACCOUNT kind: ServiceAccount namespace: fleet-system apiGroup: \"\" heartbeatPeriodSeconds: 60 EOF Set up the member agent, the Fleet component that works on the member cluster end, to enable Fleet connection:\n# Clone the Fleet repository from GitHub. git clone https://github.com/Azure/fleet.git # Install the member agent helm chart on the member cluster. # Replace the value of MEMBER_CLUSTER_CONTEXT with the name of the kubeconfig context you use # for member cluster access. export MEMBER_CLUSTER_CONTEXT=\"YOUR-MEMBER-CLUSTER-CONTEXT\" # Replace the value of HUB_CLUSTER_ADDRESS with the address of the hub cluster API server. export HUB_CLUSTER_ADDRESS=\"YOUR-HUB-CLUSTER-ADDRESS\" # The variables below uses the Fleet images kept in the Microsoft Container Registry (MCR), # and will retrieve the latest version from the Fleet GitHub repository. # # You can, however, build the Fleet images of your own; see the repository README for # more information. export REGISTRY=\"mcr.microsoft.com/aks/fleet\" export FLEET_VERSION=$(curl \"https://api.github.com/repos/Azure/fleet/tags\" | jq -r '.[0].name') export MEMBER_AGENT_IMAGE=\"member-agent\" export REFRESH_TOKEN_IMAGE=\"refresh-token\" kubectl config use-context $MEMBER_CLUSTER_CONTEXT # Create the secret with the token extracted previously for member agent to use. kubectl create secret generic hub-kubeconfig-secret --from-literal=token=$TOKEN helm install member-agent fleet/charts/member-agent/ \\ --set config.hubURL=$HUB_CLUSTER_ADDRESS \\ --set image.repository=$REGISTRY/$MEMBER_AGENT_IMAGE \\ --set image.tag=$FLEET_VERSION \\ --set refreshtoken.repository=$REGISTRY/$REFRESH_TOKEN_IMAGE \\ --set refreshtoken.tag=$FLEET_VERSION \\ --set image.pullPolicy=Always \\ --set refreshtoken.pullPolicy=Always \\ --set config.memberClusterName=\"$MEMBER_CLUSTER\" \\ --set logVerbosity=5 \\ --set namespace=fleet-system \\ --set enableV1Alpha1APIs=false \\ --set enableV1Beta1APIs=true Verify that the installation of the member agent is successful:\nkubectl get pods -n fleet-system You should see that all the returned pods are up and running. Note that it may take a few minutes for the member agent to get ready.\nVerify that the member cluster has joined the fleet successfully:\nkubectl config use-context $HUB_CLUSTER_CONTEXT kubectl get membercluster $MEMBER_CLUSTER Setting a cluster to leave a fleet Fleet uses the MemberCluster API to manage cluster memberships. To remove a member cluster from a fleet, simply delete its corresponding MemberCluster object from your hub cluster:\n# Replace the value of MEMBER-CLUSTER with the name of the member cluster you would like to # remove from a fleet. export MEMBER_CLUSTER=YOUR-MEMBER-CLUSTER kubectl delete membercluster $MEMBER_CLUSTER It may take a while before the member cluster leaves the fleet successfully. Fleet will perform some cleanup; all the resources placed onto the cluster will be removed.\nAfter the member cluster leaves, you can remove the member agent installation from it using Helm:\n# Replace the value of MEMBER_CLUSTER_CONTEXT with the name of the kubeconfig context you use # for member cluster access. export MEMBER_CLUSTER_CONTEXT=YOUR-MEMBER-CLUSTER-CONTEXT kubectl config use-context $MEMBER_CLUSTER_CONTEXT helm uninstall member-agent It may take a few moments before the uninstallation completes.\nViewing the status of a member cluster Similarly, you can use the MemberCluster API in the hub cluster to view the status of a member cluster:\n# Replace the value of MEMBER-CLUSTER with the name of the member cluster of which you would like # to view the status. export MEMBER_CLUSTER=YOUR-MEMBER-CLUSTER kubectl get membercluster $MEMBER_CLUSTER -o jsonpath=\"{.status}\" The status consists of:\nan array of conditions, including:\nthe ReadyToJoin condition, which signals whether the hub cluster is ready to accept the member cluster; and the Joined condition, which signals whether the cluster has joined the fleet; and the Healthy condition, which signals whether the cluster is in a healthy state. Typically, a member cluster should have all three conditions set to true. Refer to the Troubleshooting Guide for help if a cluster fails to join into a fleet.\nthe resource usage of the cluster; at this moment Fleet reports the capacity and the allocatable amount of each resource in the cluster, summed up from all nodes in the cluster.\nan array of agent status, which reports the status of specific Fleet agents installed in the cluster; each entry features:\nan array of conditions, in which Joined signals whether the specific agent has been successfully installed in the cluster, and Healthy signals whether the agent is in a healthy state; and the timestamp of the last received heartbeat from the agent. Adding labels to a member cluster You can add labels to a MemberCluster object in the same as with any other Kubernetes object. These labels can then be used for targeting specific clusters in resource placement. To add a label, run the command below:\n# Replace the values of MEMBER_CLUSTER, LABEL_KEY, and LABEL_VALUE with those of your own. export MEMBER_CLUSTER=YOUR-MEMBER-CLUSTER export LABEL_KEY=YOUR-LABEL-KEY export LABEL_VALUE=YOUR-LABEL-VALUE kubectl label membercluster $MEMBER_CLUSTER $LABEL_KEY=$LABEL_VALUE ","categories":"","description":"How to join or remove a cluster from a fleet, and how to view the status of and label a member cluster","excerpt":"How to join or remove a cluster from a fleet, and how to view the …","ref":"/website/docs/how-tos/clusters/","tags":"","title":"Managing clusters"},{"body":"Overview ClusterResourcePlacement concept is used to dynamically select cluster scoped resources (especially namespaces and all objects within it) and control how they are propagated to all or a subset of the member clusters. A ClusterResourcePlacement mainly consists of three parts:\nResource selection: select which cluster-scoped Kubernetes resource objects need to be propagated from the hub cluster to selected member clusters.\nIt supports the following forms of resource selection:\nSelect resources by specifying just the \u003cgroup, version, kind\u003e. This selection propagates all resources with matching \u003cgroup, version, kind\u003e. Select resources by specifying the \u003cgroup, version, kind\u003e and name. This selection propagates only one resource that matches the \u003cgroup, version, kind\u003e and name. Select resources by specifying the \u003cgroup, version, kind\u003e and a set of labels using ClusterResourcePlacement -\u003e LabelSelector. This selection propagates all resources that match the \u003cgroup, version, kind\u003e and label specified. Note: When a namespace is selected, all the namespace-scoped objects under this namespace are propagated to the selected member clusters along with this namespace.\nPlacement policy: limit propagation of selected resources to a specific subset of member clusters. The following types of target cluster selection are supported:\nPickAll (Default): select any member clusters with matching cluster Affinity scheduling rules. If the Affinity is not specified, it will select all joined and healthy member clusters. PickFixed: select a fixed list of member clusters defined in the ClusterNames. PickN: select a NumberOfClusters of member clusters with optional matching cluster Affinity scheduling rules or topology spread constraints TopologySpreadConstraints. Rollout strategy: how to propagate new changes to the selected member clusters.\nA simple ClusterResourcePlacement looks like this:\napiVersion: placement.kubernetes-fleet.io/v1 kind: ClusterResourcePlacement metadata: name: crp-1 spec: policy: placementType: PickN numberOfClusters: 2 topologySpreadConstraints: - maxSkew: 1 topologyKey: \"env\" whenUnsatisfiable: DoNotSchedule resourceSelectors: - group: \"\" kind: Namespace name: test-deployment version: v1 revisionHistoryLimit: 100 strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% unavailablePeriodSeconds: 5 type: RollingUpdate When To Use ClusterResourcePlacement ClusterResourcePlacement is useful when you want for a general way of managing and running workloads across multiple clusters. Some example scenarios include the following:\nAs a platform operator, I want to place my cluster-scoped resources (especially namespaces and all objects within it) to a cluster that resides in the us-east-1. As a platform operator, I want to spread my cluster-scoped resources (especially namespaces and all objects within it) evenly across the different regions/zones. As a platform operator, I prefer to place my test resources into the staging AKS cluster. As a platform operator, I would like to separate the workloads for compliance or policy reasons. As a developer, I want to run my cluster-scoped resources (especially namespaces and all objects within it) on 3 clusters. In addition, each time I update my workloads, the updates take place with zero downtime by rolling out to these three clusters incrementally. Placement Workflow The placement controller will create ClusterSchedulingPolicySnapshot and ClusterResourceSnapshot snapshots by watching the ClusterResourcePlacement object. So that it can trigger the scheduling and resource rollout process whenever needed.\nThe override controller will create the corresponding snapshots by watching the ClusterResourceOverride and ResourceOverride which captures the snapshot of the overrides.\nThe placement workflow will be divided into several stages:\nScheduling: multi-cluster scheduler makes the schedule decision by creating the clusterResourceBinding for a bundle of resources based on the latest ClusterSchedulingPolicySnapshotgenerated by the ClusterResourcePlacement. Rolling out resources: rollout controller applies the resources to the selected member clusters based on the rollout strategy. Overriding: work generator applies the override rules defined by ClusterResourceOverride and ResourceOverride to the selected resources on the target clusters. Creating or updating works: work generator creates the work on the corresponding member cluster namespace. Each work contains the (overridden) manifest workload to be deployed on the member clusters. Applying resources on target clusters: apply work controller applies the manifest workload on the member clusters. Checking resource availability: apply work controller checks the resource availability on the target clusters. Resource Selection Resource selectors identify cluster-scoped objects to include based on standard Kubernetes identifiers - namely, the group, kind, version, and name of the object. Namespace-scoped objects are included automatically when the namespace they are part of is selected. The example ClusterResourcePlacement above would include the test-deployment namespace and any objects that were created in that namespace.\nThe clusterResourcePlacement controller creates the ClusterResourceSnapshot to store a snapshot of selected resources selected by the placement. The ClusterResourceSnapshot spec is immutable. Each time when the selected resources are updated, the clusterResourcePlacement controller will detect the resource changes and create a new ClusterResourceSnapshot. It implies that resources can change independently of any modifications to the ClusterResourceSnapshot. In other words, resource changes can occur without directly affecting the ClusterResourceSnapshot itself.\nThe total amount of selected resources may exceed the 1MB limit for a single Kubernetes object. As a result, the controller may produce more than one ClusterResourceSnapshots for all the selected resources.\nClusterResourceSnapshot sample:\napiVersion: placement.kubernetes-fleet.io/v1 kind: ClusterResourceSnapshot metadata: annotations: kubernetes-fleet.io/number-of-enveloped-object: \"0\" kubernetes-fleet.io/number-of-resource-snapshots: \"1\" kubernetes-fleet.io/resource-hash: e0927e7d75c7f52542a6d4299855995018f4a6de46edf0f814cfaa6e806543f3 creationTimestamp: \"2023-11-10T08:23:38Z\" generation: 1 labels: kubernetes-fleet.io/is-latest-snapshot: \"true\" kubernetes-fleet.io/parent-CRP: crp-1 kubernetes-fleet.io/resource-index: \"4\" name: crp-1-4-snapshot ownerReferences: - apiVersion: placement.kubernetes-fleet.io/v1 blockOwnerDeletion: true controller: true kind: ClusterResourcePlacement name: crp-1 uid: 757f2d2c-682f-433f-b85c-265b74c3090b resourceVersion: \"1641940\" uid: d6e2108b-882b-4f6c-bb5e-c5ec5491dd20 spec: selectedResources: - apiVersion: v1 kind: Namespace metadata: labels: kubernetes.io/metadata.name: test name: test spec: finalizers: - kubernetes - apiVersion: v1 data: key1: value1 key2: value2 key3: value3 kind: ConfigMap metadata: name: test-1 namespace: test Placement Policy ClusterResourcePlacement supports three types of policy as mentioned above. ClusterSchedulingPolicySnapshot will be generated whenever policy changes are made to the ClusterResourcePlacement that require a new scheduling. Similar to ClusterResourceSnapshot, its spec is immutable.\nClusterSchedulingPolicySnapshot sample:\napiVersion: placement.kubernetes-fleet.io/v1 kind: ClusterSchedulingPolicySnapshot metadata: annotations: kubernetes-fleet.io/CRP-generation: \"5\" kubernetes-fleet.io/number-of-clusters: \"2\" creationTimestamp: \"2023-11-06T10:22:56Z\" generation: 1 labels: kubernetes-fleet.io/is-latest-snapshot: \"true\" kubernetes-fleet.io/parent-CRP: crp-1 kubernetes-fleet.io/policy-index: \"1\" name: crp-1-1 ownerReferences: - apiVersion: placement.kubernetes-fleet.io/v1 blockOwnerDeletion: true controller: true kind: ClusterResourcePlacement name: crp-1 uid: 757f2d2c-682f-433f-b85c-265b74c3090b resourceVersion: \"1639412\" uid: 768606f2-aa5a-481a-aa12-6e01e6adbea2 spec: policy: placementType: PickN policyHash: NDc5ZjQwNWViNzgwOGNmYzU4MzY2YjI2NDg2ODBhM2E4MTVlZjkxNGZlNjc1NmFlOGRmMGQ2Zjc0ODg1NDE2YQ== status: conditions: - lastTransitionTime: \"2023-11-06T10:22:56Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 1 reason: SchedulingPolicyFulfilled status: \"True\" type: Scheduled observedCRPGeneration: 5 targetClusters: - clusterName: aks-member-1 clusterScore: affinityScore: 0 priorityScore: 0 reason: picked by scheduling policy selected: true - clusterName: aks-member-2 clusterScore: affinityScore: 0 priorityScore: 0 reason: picked by scheduling policy selected: true In contrast to the original scheduler framework in Kubernetes, the multi-cluster scheduling process involves selecting a cluster for placement through a structured 5-step operation:\nBatch \u0026 PostBatch Filter Score Sort Bind The batch \u0026 postBatch step is to define the batch size according to the desired and current ClusterResourceBinding. The postBatch is to adjust the batch size if needed.\nThe filter step finds the set of clusters where it’s feasible to schedule the placement, for example, whether the cluster is matching required Affinity scheduling rules specified in the Policy. It also filters out any clusters which are leaving the fleet or no longer connected to the fleet, for example, its heartbeat has been stopped for a prolonged period of time.\nIn the score step (only applied to the pickN type), the scheduler assigns a score to each cluster that survived filtering. Each cluster is given a topology spread score (how much a cluster would satisfy the topology spread constraints specified by the user), and an affinity score (how much a cluster would satisfy the preferred affinity terms specified by the user).\nIn the sort step (only applied to the pickN type), it sorts all eligible clusters by their scores, sorting first by topology spread score and breaking ties based on the affinity score.\nThe bind step is to create/update/delete the ClusterResourceBinding based on the desired and current member cluster list.\nRollout Strategy Update strategy determines how changes to the ClusterWorkloadPlacement will be rolled out across member clusters. The only supported update strategy is RollingUpdate and it replaces the old placed resource using rolling update, i.e. gradually create the new one while replace the old ones.\nPlacement status After a ClusterResourcePlacement is created, details on current status can be seen by performing a kubectl describe crp \u003cname\u003e. The status output will indicate both placement conditions and individual placement statuses on each member cluster that was selected. The list of resources that are selected for placement will also be included in the describe output.\nSample output:\nName: crp-1 Namespace: Labels: \u003cnone\u003e Annotations: \u003cnone\u003e API Version: placement.kubernetes-fleet.io/v1 Kind: ClusterResourcePlacement Metadata: ... Spec: Policy: Placement Type: PickAll Resource Selectors: Group: Kind: Namespace Name: application-1 Version: v1 Revision History Limit: 10 Strategy: Rolling Update: Max Surge: 25% Max Unavailable: 25% Unavailable Period Seconds: 2 Type: RollingUpdate Status: Conditions: Last Transition Time: 2024-04-29T09:58:20Z Message: found all the clusters needed as specified by the scheduling policy Observed Generation: 1 Reason: SchedulingPolicyFulfilled Status: True Type: ClusterResourcePlacementScheduled Last Transition Time: 2024-04-29T09:58:20Z Message: All 3 cluster(s) start rolling out the latest resource Observed Generation: 1 Reason: RolloutStarted Status: True Type: ClusterResourcePlacementRolloutStarted Last Transition Time: 2024-04-29T09:58:20Z Message: No override rules are configured for the selected resources Observed Generation: 1 Reason: NoOverrideSpecified Status: True Type: ClusterResourcePlacementOverridden Last Transition Time: 2024-04-29T09:58:20Z Message: Works(s) are succcesfully created or updated in the 3 target clusters' namespaces Observed Generation: 1 Reason: WorkSynchronized Status: True Type: ClusterResourcePlacementWorkSynchronized Last Transition Time: 2024-04-29T09:58:20Z Message: The selected resources are successfully applied to 3 clusters Observed Generation: 1 Reason: ApplySucceeded Status: True Type: ClusterResourcePlacementApplied Last Transition Time: 2024-04-29T09:58:20Z Message: The selected resources in 3 cluster are available now Observed Generation: 1 Reason: ResourceAvailable Status: True Type: ClusterResourcePlacementAvailable Observed Resource Index: 0 Placement Statuses: Cluster Name: kind-cluster-1 Conditions: Last Transition Time: 2024-04-29T09:58:20Z Message: Successfully scheduled resources for placement in kind-cluster-1 (affinity score: 0, topology spread score: 0): picked by scheduling policy Observed Generation: 1 Reason: Scheduled Status: True Type: Scheduled Last Transition Time: 2024-04-29T09:58:20Z Message: Detected the new changes on the resources and started the rollout process Observed Generation: 1 Reason: RolloutStarted Status: True Type: RolloutStarted Last Transition Time: 2024-04-29T09:58:20Z Message: No override rules are configured for the selected resources Observed Generation: 1 Reason: NoOverrideSpecified Status: True Type: Overridden Last Transition Time: 2024-04-29T09:58:20Z Message: All of the works are synchronized to the latest Observed Generation: 1 Reason: AllWorkSynced Status: True Type: WorkSynchronized Last Transition Time: 2024-04-29T09:58:20Z Message: All corresponding work objects are applied Observed Generation: 1 Reason: AllWorkHaveBeenApplied Status: True Type: Applied Last Transition Time: 2024-04-29T09:58:20Z Message: The availability of work object crp-1-work is not trackable Observed Generation: 1 Reason: WorkNotTrackable Status: True Type: Available Cluster Name: kind-cluster-2 Conditions: Last Transition Time: 2024-04-29T09:58:20Z Message: Successfully scheduled resources for placement in kind-cluster-2 (affinity score: 0, topology spread score: 0): picked by scheduling policy Observed Generation: 1 Reason: Scheduled Status: True Type: Scheduled Last Transition Time: 2024-04-29T09:58:20Z Message: Detected the new changes on the resources and started the rollout process Observed Generation: 1 Reason: RolloutStarted Status: True Type: RolloutStarted Last Transition Time: 2024-04-29T09:58:20Z Message: No override rules are configured for the selected resources Observed Generation: 1 Reason: NoOverrideSpecified Status: True Type: Overridden Last Transition Time: 2024-04-29T09:58:20Z Message: All of the works are synchronized to the latest Observed Generation: 1 Reason: AllWorkSynced Status: True Type: WorkSynchronized Last Transition Time: 2024-04-29T09:58:20Z Message: All corresponding work objects are applied Observed Generation: 1 Reason: AllWorkHaveBeenApplied Status: True Type: Applied Last Transition Time: 2024-04-29T09:58:20Z Message: The availability of work object crp-1-work is not trackable Observed Generation: 1 Reason: WorkNotTrackable Status: True Type: Available Cluster Name: kind-cluster-3 Conditions: Last Transition Time: 2024-04-29T09:58:20Z Message: Successfully scheduled resources for placement in kind-cluster-3 (affinity score: 0, topology spread score: 0): picked by scheduling policy Observed Generation: 1 Reason: Scheduled Status: True Type: Scheduled Last Transition Time: 2024-04-29T09:58:20Z Message: Detected the new changes on the resources and started the rollout process Observed Generation: 1 Reason: RolloutStarted Status: True Type: RolloutStarted Last Transition Time: 2024-04-29T09:58:20Z Message: No override rules are configured for the selected resources Observed Generation: 1 Reason: NoOverrideSpecified Status: True Type: Overridden Last Transition Time: 2024-04-29T09:58:20Z Message: All of the works are synchronized to the latest Observed Generation: 1 Reason: AllWorkSynced Status: True Type: WorkSynchronized Last Transition Time: 2024-04-29T09:58:20Z Message: All corresponding work objects are applied Observed Generation: 1 Reason: AllWorkHaveBeenApplied Status: True Type: Applied Last Transition Time: 2024-04-29T09:58:20Z Message: The availability of work object crp-1-work is not trackable Observed Generation: 1 Reason: WorkNotTrackable Status: True Type: Available Selected Resources: Kind: Namespace Name: application-1 Version: v1 Kind: ConfigMap Name: app-config-1 Namespace: application-1 Version: v1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal PlacementRolloutStarted 3m46s cluster-resource-placement-controller Started rolling out the latest resources Normal PlacementOverriddenSucceeded 3m46s cluster-resource-placement-controller Placement has been successfully overridden Normal PlacementWorkSynchronized 3m46s cluster-resource-placement-controller Work(s) have been created or updated successfully for the selected cluster(s) Normal PlacementApplied 3m46s cluster-resource-placement-controller Resources have been applied to the selected cluster(s) Normal PlacementRolloutCompleted 3m46s cluster-resource-placement-controller Resources are available in the selected clusters Tolerations Tolerations are a mechanism to allow the Fleet Scheduler to schedule resources to a MemberCluster that has taints specified on it. We adopt the concept of taints \u0026 tolerations introduced in Kubernetes to the multi-cluster use case.\nThe ClusterResourcePlacement CR supports the specification of list of tolerations, which are applied to the ClusterResourcePlacement object. Each Toleration object comprises the following fields:\nkey: The key of the toleration. value: The value of the toleration. effect: The effect of the toleration, which can be NoSchedule for now. operator: The operator of the toleration, which can be Exists or Equal. Each toleration is used to tolerate one or more specific taints applied on the MemberCluster. Once all taints on a MemberCluster are tolerated by tolerations on a ClusterResourcePlacement, resources can be propagated to the MemberCluster by the scheduler for that ClusterResourcePlacement resource.\nTolerations cannot be updated or removed from a ClusterResourcePlacement. If there is a need to update toleration a better approach is to add another toleration. If we absolutely need to update or remove existing tolerations, the only option is to delete the existing ClusterResourcePlacement and create a new object with the updated tolerations.\nFor detailed instructions, please refer to this document.\nEnvelope Object The ClusterResourcePlacement leverages the fleet hub cluster as a staging environment for customer resources. These resources are then propagated to member clusters that are part of the fleet, based on the ClusterResourcePlacement spec.\nIn essence, the objective is not to apply or create resources on the hub cluster for local use but to propagate these resources to other member clusters within the fleet.\nCertain resources, when created or applied on the hub cluster, may lead to unintended side effects. These include:\nValidating/Mutating Webhook Configurations Cluster Role Bindings Resource Quotas Storage Classes Flow Schemas Priority Classes Ingress Classes Ingresses Network Policies To address this, we support the use of ConfigMap with a fleet-reserved annotation. This allows users to encapsulate resources that might have side effects on the hub cluster within the ConfigMap. For detailed instructions, please refer to this document.\n","categories":"","description":"Concept about the ClusterResourcePlacement API","excerpt":"Concept about the ClusterResourcePlacement API","ref":"/website/docs/concepts/crpc/","tags":"","title":"ClusterResourcePlacement"},{"body":"In this tutorial, you will try Fleet out using KinD clusters, which are Kubernetes clusters running on your own local machine via Docker containers. This is the easiest way to get started with Fleet, which can help you understand how Fleet simiplify the day-to-day multi-cluster management experience with very little setup needed.\nNote\nkind is a tool for setting up a Kubernetes environment for experimental purposes; some instructions below for running Fleet in the kind environment may not apply to other environments, and there might also be some minor differences in the Fleet experience.\nBefore you begin To complete this tutorial, you will need:\nThe following tools on your local machine: kind, for running Kubernetes clusters on your local machine Docker git curl helm, the Kubernetes package manager jq base64 Spin up a few kind clusters The Fleet open-source project manages a multi-cluster environment using a hub-spoke pattern, which consists of one hub cluster and one or more member clusters:\nThe hub cluster is the portal to which every member cluster connects; it also serves as an interface for centralized management, through which you can perform a number of tasks, primarily orchestrating workloads across different clusters. A member cluster connects to the hub cluster and runs your workloads as orchestrated by the hub cluster. In this tutorial you will create two kind clusters; one of which serves as the Fleet hub cluster, and the other the Fleet member cluster. Run the commands below to create them:\n# Replace YOUR-KIND-IMAGE with a kind node image name of your # choice. It should match with the version of kind installed # on your system; for more information, see # [kind releases](https://github.com/kubernetes-sigs/kind/releases). export KIND_IMAGE=YOUR-KIND-IMAGE # Replace YOUR-KUBECONFIG-PATH with the path to a Kubernetes # configuration file of your own, typically $HOME/.kube/config. export KUBECONFIG_PATH=YOUR-KUBECONFIG-PATH # The names of the kind clusters; you may use values of your own if you'd like to. export HUB_CLUSTER=hub export MEMBER_CLUSTER=member-1 kind create cluster --name $HUB_CLUSTER \\ --image=$KIND_IMAGE \\ --kubeconfig=$KUBECONFIG_PATH kind create cluster --name $MEMBER_CLUSTER \\ --image=$KIND_IMAGE \\ --kubeconfig=$KUBECONFIG_PATH # Export the configurations for the kind clusters. kind export kubeconfig -n $HUB_CLUSTER kind export kubeconfig -n $MEMBER_CLUSTER Set up the Fleet hub cluster To set up the hub cluster, run the commands below:\nexport HUB_CLUSTER_CONTEXT=kind-$HUB_CLUSTER kubectl config use-context $HUB_CLUSTER_CONTEXT # The variables below uses the Fleet images kept in the Microsoft Container Registry (MCR), # and will retrieve the latest version from the Fleet GitHub repository. # # You can, however, build the Fleet images of your own; see the repository README for # more information. export REGISTRY=\"mcr.microsoft.com/aks/fleet\" export FLEET_VERSION=$(curl \"https://api.github.com/repos/Azure/fleet/tags\" | jq -r '.[0].name') export HUB_AGENT_IMAGE=\"hub-agent\" # Clone the Fleet repository from GitHub. git clone https://github.com/Azure/fleet.git # Install the helm chart for running Fleet agents on the hub cluster. helm install hub-agent fleet/charts/hub-agent/ \\ --set image.pullPolicy=Always \\ --set image.repository=$REGISTRY/$HUB_AGENT_IMAGE \\ --set image.tag=$FLEET_VERSION \\ --set logVerbosity=2 \\ --set namespace=fleet-system \\ --set enableWebhook=true \\ --set webhookClientConnectionType=service \\ --set enableV1Alpha1APIs=false \\ --set enableV1Beta1APIs=true It may take a few seconds for the installation to complete. Once it finishes, verify that the Fleet hub agents are up and running with the commands below:\nkubectl get pods -n fleet-system You should see that all the pods are in the ready state.\nSet up the Fleet member custer Next, you will set up the other kind cluster you created earlier as the Fleet member cluster, which requires that you install the Fleet member agent on the cluster and connect it to the Fleet hub cluster.\nFor your convenience, Fleet provides a script that can automate the process of joining a cluster into a fleet. To use the script, follow the steps below:\n# Query the API server address of the hub cluster. export HUB_CLUSTER_ADDRESS=\"https://$(docker inspect $HUB_CLUSTER-control-plane --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'):6443\" export MEMBER_CLUSTER_CONTEXT=kind-$MEMBER_CLUSTER # Run the script. chmod +x fleet/hack/membership/join.sh ./fleet/hack/membership/join.sh It may take a few minutes for the script to finish running. Once it is completed, verify that the cluster has joined successfully with the command below:\nkubectl config use-context $HUB_CLUSTER_CONTEXT kubectl get membercluster $MEMBER_CLUSTER The newly joined cluster should have the JOINED status field set to True. If you see that the cluster is still in an unknown state, it might be that the member cluster is still connecting to the hub cluster. Should this state persist for a prolonged period, refer to the Troubleshooting Guide for more information.\nNote\nIf you would like to know more about the steps the script runs, or would like to join a cluster into a fleet manually, refer to the Managing Clusters How-To Guide.\nUse the ClusterResourcePlacement API to orchestrate resources among member clusters. Fleet offers an API, ClusterResourcePlacement, which helps orchestrate workloads, i.e., any group Kubernetes resources, among all member clusters. In this last part of the tutorial, you will use this API to place some Kubernetes resources automatically into the member clusters via the hub cluster, saving the trouble of having to create them one by one in each member cluster.\nCreate the resources for placement Run the commands below to create a namespace and a config map, which will be placed onto the member clusters.\nkubectl create namespace work kubectl create configmap app -n work --from-literal=data=test It may take a few seconds for the commands to complete.\nCreate the ClusterResourcePlacement API object Next, create a ClusterResourcePlacement API object in the hub cluster:\nkubectl apply -f - \u003c\u003cEOF apiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - group: \"\" kind: Namespace version: v1 name: work policy: placementType: PickAll EOF Note that the CRP object features a resource selector, which targets the work namespace you just created. This will instruct the CRP to place the namespace itself, and all resources registered under the namespace, such as the config map, to the target clusters. Also, in the policy field, a PickAll placement type has been specified. This allows the CRP to automatically perform the placement on all member clusters in the fleet, including those that join after the CRP object is created.\nIt may take a few seconds for Fleet to successfully place the resources. To check up on the progress, run the commands below:\nkubectl get clusterresourceplacement crp Verify that the placement has been completed successfully; you should see that the APPLIED status field has been set to True. You may need to repeat the commands a few times to wait for the completion.\nConfirm the placement Now, log into the member clusters to confirm that the placement has been completed.\nkubectl config use-context $MEMBER_CLUSTER_CONTEXT kubectl get ns kubectl get configmap -n work You should see the namespace work and the config map app listed in the output.\nClean things up To remove all the resources you just created, run the commands below:\n# This would also remove the namespace and config map placed in all member clusters. kubectl delete crp crp kubectl delete ns work kubectl delete configmap app -n work To uninstall Fleet, run the commands below:\nkubectl config use-context $HUB_CLUSTER_CONTEXT helm uninstall hub-agent kubectl config use-context $MEMBER_CLUSTER_CONTEXT helm uninstall member-agent What’s next Congratulations! You have completed the getting started tutorial for Fleet. To learn more about Fleet:\nRead about Fleet concepts Read about the ClusterResourcePlacement API Read the Fleet API reference ","categories":"","description":"Use KinD clusters to learn about Fleet","excerpt":"Use KinD clusters to learn about Fleet","ref":"/website/docs/getting-started/kind/","tags":"","title":"Getting started with Fleet using KinD clusters"},{"body":"This guide provides an overview of how to use the Fleet ClusterResourcePlacement (CRP) API to orchestrate workload distribution across your fleet.\nOverview The CRP API is a core Fleet API that facilitates the distribution of specific resources from the hub cluster to member clusters within a fleet. This API offers scheduling capabilities that allow you to target the most suitable group of clusters for a set of resources using a complex rule set. For example, you can distribute resources to clusters in specific regions (North America, East Asia, Europe, etc.) and/or release stages (production, canary, etc.). You can even distribute resources according to certain topology spread constraints.\nAPI Components The CRP API generally consists of the following components:\nResource Selectors: These specify the set of resources selected for placement. Scheduling Policy: This determines the set of clusters where the resources will be placed. Rollout Strategy: This controls the behavior of resource placement when the resources themselves and/or the scheduling policy are updated, minimizing interruptions caused by refreshes. The following sections discuss these components in depth.\nResource selectors A ClusterResourcePlacement object may feature one or more resource selectors, specifying which resources to select for placement. To add a resource selector, edit the resourceSelectors field in the ClusterResourcePlacement spec:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - group: \"rbac.authorization.k8s.io\" kind: ClusterRole version: v1 name: secretReader The example above will pick a ClusterRole named secretReader for resource placement.\nIt is important to note that, as its name implies, ClusterResourcePlacement selects only cluster-scoped resources. However, if you select a namespace, all the resources under the namespace will also be placed.\nDifferent types of resource selectors You can specify a resource selector in many different ways:\nTo select one specific resource, such as a namespace, specify its API GVK (group, version, and kind), and its name, in the resource selector:\n# As mentioned earlier, all the resources under the namespace will also be selected. resourceSelectors: - group: \"\" kind: Namespace version: v1 name: work Alternately, you may also select a set of resources of the same API GVK using a label selector; it also requires that you specify the API GVK and the filtering label(s):\n# As mentioned earlier, all the resources under the namespaces will also be selected. resourceSelectors: - group: \"\" kind: Namespace version: v1 labelSelector: matchLabels: system: critical In the example above, all the namespaces in the hub cluster with the label system=critical will be selected (along with the resources under them).\nFleet uses standard Kubernetes label selectors; for its specification and usage, see the Kubernetes API reference.\nVery occasionally, you may need to select all the resources under a specific GVK; to achieve this, use a resource selector with only the API GVK added:\nresourceSelectors: - group: \"rbac.authorization.k8s.io\" kind: ClusterRole version: v1 In the example above, all the cluster roles in the hub cluster will be picked.\nMultiple resource selectors You may specify up to 100 different resource selectors; Fleet will pick a resource if it matches any of the resource selectors specified (i.e., all selectors are OR’d).\n# As mentioned earlier, all the resources under the namespace will also be selected. resourceSelectors: - group: \"\" kind: Namespace version: v1 name: work - group: \"rbac.authorization.k8s.io\" kind: ClusterRole version: v1 name: secretReader In the example above, Fleet will pick the namespace work (along with all the resources under it) and the cluster role secretReader.\nNote\nYou can find the GVKs of built-in Kubernetes API objects in the Kubernetes API reference.\nScheduling policy Each scheduling policy is associated with a placement type, which determines how Fleet will pick clusters. The ClusterResourcePlacement API supports the following placement types:\nPlacement type Description PickFixed Pick a specific set of clusters by their names. PickAll Pick all the clusters in the fleet, per some standard. PickN Pick a count of N clusters in the fleet, per some standard. Note\nScheduling policy itself is optional. If you do not specify a scheduling policy, Fleet will assume that you would like to use a scheduling of the PickAll placement type; it effectively sets Fleet to pick all the clusters in the fleet.\nFleet does not support switching between different placement types; if you need to do so, re-create a new ClusterResourcePlacement object.\nPickFixed placement type PickFixed is the most straightforward placement type, through which you directly tell Fleet which clusters to place resources at. To use this placement type, specify the target cluster names in the clusterNames field, such as\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickFixed clusterNames: - bravelion - smartfish The example above will place resources to two clusters, bravelion and smartfish.\nPickAll placement type PickAll placement type allows you to pick all clusters in the fleet per some standard. With this placement type, you may use affinity terms to fine-tune which clusters you would like for Fleet to pick:\nAn affinity term specifies a requirement that a cluster needs to meet, usually the presence of a label.\nThere are two types of affinity terms:\nrequiredDuringSchedulingIgnoredDuringExecution terms are requirements that a cluster must meet before it can be picked; and preferredDuringSchedulingIgnoredDuringExecution terms are requirements that, if a cluster meets, will set Fleet to prioritize it in scheduling. In the scheduling policy of the PickAll placement type, you may only use the requiredDuringSchedulingIgnoredDuringExecution terms.\nNote\nYou can learn more about affinities in Using Affinities to Pick Clusters How-To Guide.\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: system: critical The ClusterResourcePlacement object above will pick all the clusters with the label system:critical on them; clusters without the label will be ignored.\nFleet is forward-looking with the PickAll placement type: any cluster that satisfies the affinity terms of a ClusterResourcePlacement object, even if it joins after the ClsuterResourcePlacement object is created, will be picked.\nNote\nYou may specify a scheduling policy of the PickAll placement with no affinity; this will set Fleet to select all clusters currently present in the fleet.\nPickN placement type PickN placement type allows you to pick a specific number of clusters in the fleet for resource placement; with this placement type, you may use affinity terms and topology spread constraints to fine-tune which clusters you would like Fleet to pick.\nAn affinity term specifies a requirement that a cluster needs to meet, usually the presence of a label.\nThere are two types of affinity terms:\nrequiredDuringSchedulingIgnoredDuringExecution terms are requirements that a cluster must meet before it can be picked; and preferredDuringSchedulingIgnoredDuringExecution terms are requirements that, if a cluster meets, will set Fleet to prioritize it in scheduling. A topology spread constraint can help you spread resources evenly across different groups of clusters. For example, you may want to have a database replica deployed in each region to enable high-availability.\nNote\nYou can learn more about affinities in Using Affinities to Pick Clusters How-To Guide, and more about topology spread constraints in Using Topology Spread Constraints to Pick Clusters How-To Guide.\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 3 affinity: clusterAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 preference: - labelSelector: matchLabels: critical-level: 1 The ClusterResourcePlacement object above will pick first clusters with the critical-level=1 on it; if only there are not enough (less than 3) such clusters, will Fleet pick clusters with no such label.\nTo be more precise, with this placement type, Fleet scores clusters on how well it satisfies the affinity terms and the topology spread constraints; Fleet will assign:\nan affinity score, for how well the cluster satisfies the affinity terms; and a topology spread score, for how well the cluster satisfies the topology spread constraints. Note\nFor more information on the scoring specifics, see Using Affinities to Pick Clusters How-To Guide (for affinity score) and Using Topology Spread Constraints to Pick Clusters How-To Guide (for topology spread score).\nAfter scoring, Fleet ranks the clusters using the rule below and picks the top N clusters:\nthe cluster with the highest topology spread score ranks the highest;\nif there are multiple clusters with the same topology spread score, the one with the highest affinity score ranks the highest;\nif there are multiple clusters with same topology spread score and affinity score, sort their names by alphanumeric order; the one with the most significant name ranks the highest.\nThis helps establish deterministic scheduling behavior.\nBoth affinity terms and topology spread constraints are optional. If you do not specify affinity terms or topology spread constraints, all clusters will be assigned 0 in affinity score or topology spread score respectively. When neither is added in the scheduling policy, Fleet will simply rank clusters by their names, and pick N out of them, with most significant names in alphanumeric order.\nWhen there are not enough clusters to pick It may happen that Fleet cannot find enough clusters to pick. In this situation, Fleet will keep looking until all N clusters are found.\nNote that Fleet will stop looking once all N clusters are found, even if there appears a cluster that scores higher.\nUp-scaling and downscaling You can edit the numberOfClusters field in the scheduling policy to pick more or less clusters. When up-scaling, Fleet will score all the clusters that have not been picked earlier, and find the most appropriate ones; for downscaling, Fleet will unpick the clusters that ranks lower first.\nNote\nFor downscaling, the ranking Fleet uses for unpicking clusters is composed when the scheduling is performed, i.e., it may not reflect the latest setup in the Fleet.\nA few more points about scheduling policies Responding to changes in the fleet Generally speaking, once a cluster is picked by Fleet for a ClusterResourcePlacement object, it will not be unpicked even if you modify the cluster in a way that renders it unfit for the scheduling policy, e.g., you have removed a label for the cluster, which is required for some affinity term. Fleet will also not remove resources from the cluster even if the cluster becomes unhealthy, e.g., it gets disconnected from the hub cluster. This helps reduce service interruption.\nHowever, Fleet will unpick a cluster if it leaves the fleet. If you are using a scheduling policy of the PickN placement type, Fleet will attempt to find a new cluster as replacement.\nFinding the scheduling decisions Fleet makes You can find out why Fleet picks a cluster in the status of a ClusterResourcePlacement object. For more information, see the Understanding the Status of a ClusterResourcePlacement How-To Guide.\nAvailable fields for each placement type The table below summarizes the available scheduling policy fields for each placement type:\nPickFixed PickAll PickN placementType ✅ ✅ ✅ numberOfClusters ❌ ❌ ✅ clusterNames ✅ ❌ ❌ affinity ❌ ✅ ✅ topologySpreadConstraints ❌ ❌ ✅ Rollout strategy After a ClusterResourcePlacement is created, you may want to\nAdd, update, or remove the resources that have been selected by the ClusterResourcePlacement in the hub cluster Update the resource selectors in the ClusterResourcePlacement Update the scheduling policy in the ClusterResourcePlacement These changes may trigger the following outcomes:\nNew resources may need to be placed on all picked clusters Resources already placed on a picked cluster may get updated or deleted Some clusters picked previously are now unpicked, and resources must be removed from such clusters Some clusters are newly picked, and resources must be added to them Most outcomes can lead to service interruptions. Apps running on member clusters may temporarily become unavailable as Fleet dispatches updated resources. Clusters that are no longer selected will lose all placed resources, resulting in lost traffic. If too many new clusters are selected and Fleet places resources on them simultaneously, your backend may become overloaded. The exact interruption pattern may vary depending on the resources you place using Fleet.\nTo minimize interruption, Fleet allows users to configure the rollout strategy, similar to native Kubernetes deployment, to transition between changes as smoothly as possible. Currently, Fleet supports only one rollout strategy: rolling update. This strategy ensures changes, including the addition or removal of selected clusters and resource refreshes, are applied incrementally in a phased manner at a pace suitable for you. This is the default option and applies to all changes you initiate.\nThis rollout strategy can be configured with the following parameters:\nmaxUnavailable determines how many clusters may become unavailable during a change for the selected set of resources. It can be set as an absolute number or a percentage. The default is 25%, and zero should not be used for this value.\nSetting this parameter to a lower value will result in less interruption during a change but will lead to slower rollouts.\nFleet considers a cluster as unavailable if resources have not been successfully applied to the cluster.\nHow Fleet interprets this value Fleet, in actuality, makes sure that at any time, there are **at least** N - `maxUnavailable` number of clusters available, where N is: for scheduling policies of the PickN placement type, the numberOfClusters value given; for scheduling policies of the PickFixed placement type, the number of cluster names given; for scheduling policies of the PickAll placement type, the number of clusters Fleet picks. If you use a percentage for the maxUnavailable parameter, it is calculated against N as well.\nmaxSurge determines the number of additional clusters, beyond the required number, that will receive resource placements. It can also be set as an absolute number or a percentage. The default is 25%, and zero should not be used for this value.\nSetting this parameter to a lower value will result in fewer resource placements on additional clusters by Fleet, which may slow down the rollout process.\nHow Fleet interprets this value Fleet, in actuality, makes sure that at any time, there are **at most** N + `maxSurge` number of clusters available, where N is: for scheduling policies of the PickN placement type, the numberOfClusters value given; for scheduling policies of the PickFixed placement type, the number of cluster names given; for scheduling policies of the PickAll placement type, the number of clusters Fleet picks. If you use a percentage for the maxUnavailable parameter, it is calculated against N as well.\nunavailablePeriodSeconds allows users to inform the fleet when the resources are deemed “ready”. The default value is 60 seconds.\nFleet only considers newly applied resources on a cluster as “ready” once unavailablePeriodSeconds seconds have passed after the resources have been successfully applied to that cluster. Setting a lower value for this parameter will result in faster rollouts. However, we strongly recommend that users set it to a value that all the initialization/preparation tasks can be completed within that time frame. This ensures that the resources are typically ready after the unavailablePeriodSeconds have passed. We are currently designing a generic “ready gate” for resources being applied to clusters. Please feel free to raise issues or provide feedback if you have any thoughts on this. Note\nFleet will round numbers up if you use a percentage for maxUnavailable and/or maxSurge.\nFor example, if you have a ClusterResourcePlacement with a scheduling policy of the PickN placement type and a target number of clusters of 10, with the default rollout strategy, as shown in the example below,\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: ... strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% unavailablePeriodSeconds: 60 Every time you initiate a change on selected resources, Fleet will:\nFind 10 * 25% = 2.5, rounded up to 3 clusters, which will receive the resource refresh; Wait for 60 seconds (unavailablePeriodSeconds), and repeat the process; Stop when all the clusters have received the latest version of resources. The exact period of time it takes for Fleet to complete a rollout depends not only on the unavailablePeriodSeconds, but also the actual condition of a resource placement; that is, if it takes longer for a cluster to get the resources applied successfully, Fleet will wait longer to complete the rollout, in accordance with the rolling update strategy you specified.\nNote\nIn very extreme circumstances, rollout may get stuck, if Fleet just cannot apply resources to some clusters. You can identify this behavior if CRP status; for more information, see Understanding the Status of a ClusterResourcePlacement How-To Guide.\nSnapshots and revisions Internally, Fleet keeps a history of all the scheduling policies you have used with a ClusterResourcePlacement, and all the resource versions (snapshots) the ClusterResourcePlacement has selected. These are kept as ClusterSchedulingPolicySnapshot and ClusterResourceSnapshot objects respectively.\nYou can list and view such objects for reference, but you should not modify their contents (in a typical setup, such requests will be rejected automatically). To control the length of the history (i.e., how many snapshot objects Fleet will keep for a ClusterResourcePlacement), configure the revisionHistoryLimit field:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: ... strategy: ... revisionHistoryLimit: 10 The default value is 10.\nNote\nIn this early stage, the history is kept for reference purposes only; in the future, Fleet may add features to allow rolling back to a specific scheduling policy and/or resource version.\n","categories":"","description":"How to use the `ClusterResourcePlacement` API","excerpt":"How to use the `ClusterResourcePlacement` API","ref":"/website/docs/how-tos/crp/","tags":"","title":"Using the ClusterResourcePlacement API"},{"body":"Components This document provides an overview of the components required for a fully functional and operational Fleet setup.\nThe fleet consists of the following components:\nfleet-hub-agent is a Kubernetes controller that creates and reconciles all the fleet related CRs in the hub cluster. fleet-member-agent is a Kubernetes controller that creates and reconciles all the fleet related CRs in the member cluster. The fleet-member-agent is pulling the latest CRs from the hub cluster and consistently reconciles the member clusters to the desired state. The fleet implements agent-based pull mode. So that the working pressure can be distributed to the member clusters, and it helps to breach the bottleneck of scalability, by dividing the load into each member cluster. On the other hand, hub cluster does not need to directly access to the member clusters. Fleet can support the member clusters which only have the outbound network and no inbound network access.\nTo allow multiple clusters to run securely, fleet will create a reserved namespace on the hub cluster to isolate the access permissions and resources across multiple clusters.\n","categories":"","description":"Concept about the Fleet components","excerpt":"Concept about the Fleet components","ref":"/website/docs/concepts/components/","tags":"","title":"Fleet components"},{"body":"In this tutorial, you will try Fleet out using a few of your own Kubernetes clusters; Fleet can help you manage workloads seamlessly across these clusters, greatly simplifying the experience of day-to-day Kubernetes management.\nNote\nThis tutorial assumes that you have some experience of performing administrative tasks for Kubernetes clusters. If you are just gettings started with Kubernetes, or do not have much experience of setting up a Kubernetes cluster, it is recommended that you follow the Getting started with Fleet using Kind clusters tutorial instead.\nBefore you begin To complete this tutorial, you will need:\nAt least two Kubernetes clusters of your own. Note that one of these clusters will serve as your hub cluster; other clusters must be able to reach it via the network. The following tools on your local machine: kubectl, the Kubernetes CLI tool. git curl helm, the Kubernetes package manager jq base64 Set up a Fleet hub cluster The Fleet open-source project manages a multi-cluster environment using a hub-spoke pattern, which consists of one hub cluster and one or more member clusters:\nThe hub cluster is the portal to which every member cluster connects; it also serves as an interface for centralized management, through which you can perform a number of tasks, primarily orchestrating workloads across different clusters. A member cluster connects to the hub cluster and runs your workloads as orchestrated by the hub cluster. Any Kubernetes cluster running a supported version of Kubernetes can serve as the hub cluster; it is recommended that you reserve a cluster specifically for this responsibility, and do not run other workloads on it. For the best experience, consider disabling the built-in kube-controller-manager controllers for the cluster: you could achieve this by setting the --controllers CLI argument; for more information, see the kube-controller-manager documentation.\nTo set up the hub cluster, run the commands below:\n# Replace YOUR-HUB-CLUSTER-CONTEXT with the name of the kubeconfig context for your hub cluster. export HUB_CLUSTER_CONTEXT=YOUR-HUB-CLUSTER-CONTEXT kubectl config use-context $HUB_CLUSTER_CONTEXT # The variables below uses the Fleet images kept in the Microsoft Container Registry (MCR), # and will retrieve the latest version from the Fleet GitHub repository. # # You can, however, build the Fleet images of your own; see the repository README for # more information. export REGISTRY=\"mcr.microsoft.com/aks/fleet\" export FLEET_VERSION=$(curl \"https://api.github.com/repos/Azure/fleet/tags\" | jq -r '.[0].name') export HUB_AGENT_IMAGE=\"hub-agent\" # Clone the Fleet repository from GitHub. git clone https://github.com/Azure/fleet.git # Install the helm chart for running Fleet agents on the hub cluster. helm install hub-agent fleet/charts/hub-agent/ \\ --set image.pullPolicy=Always \\ --set image.repository=$REGISTRY/$HUB_AGENT_IMAGE \\ --set image.tag=$FLEET_VERSION \\ --set logVerbosity=2 \\ --set namespace=fleet-system \\ --set enableWebhook=true \\ --set webhookClientConnectionType=service \\ --set enableV1Alpha1APIs=false \\ --set enableV1Beta1APIs=true It may take a few seconds for the installation to complete. Once it finishes, verify that the Fleet hub agents are up and running with the commands below:\nkubectl get pods -n fleet-system You should see that all the pods are in the ready state.\nConnect a member cluster to the hub cluster Next, you will set up a cluster as the member cluster for your fleet. This cluster should run a supported version of Kubernetes and be able to connect to the hub cluster via the network.\nFor your convenience, Fleet provides a script that can automate the process of joining a cluster into a fleet. To use the script, follow the steps below:\n# Replace the value of HUB_CLUSTER_ADDRESS with the address of your hub cluster API server. export HUB_CLUSTER_ADDRESS=YOUR-HUB-CLUSTER-ADDRESS # Replace the value of MEMBER_CLUSTER with the name you would like to assign to the new member # cluster. # # Note that Fleet will recognize your cluster with this name once it joins. export MEMBER_CLUSTER=YOUR-MEMBER-CLUSTER # Replace the value of MEMBER_CLUSTER_CONTEXT with the name of the kubeconfig context you use # for accessing your member cluster. export MEMBER_CLUSTER_CONTEXT=YOUR-MEMBER-CLUSTER-CONTEXT # Run the script. chmod +x fleet/hack/membership/join.sh ./fleet/hack/membership/join.sh It may take a few minutes for the script to finish running. Once it is completed, verify that the cluster has joined successfully with the command below:\nkubectl config use-context $HUB_CLUSTER_CONTEXT kubectl get membercluster $MEMBER_CLUSTER The newly joined cluster should have the JOINED status field set to True. If you see that the cluster is still in an unknown state, it might be that the member cluster is still connecting to the hub cluster. Should this state persist for a prolonged period, refer to the Troubleshooting Guide for more information.\nNote\nIf you would like to know more about the steps the script runs, or would like to join a cluster into a fleet manually, refer to the Managing Clusters How-To Guide.\nRepeat the steps above to join more clusters into your fleet.\nUse the ClusterResourcePlacement API to orchestrate resources among member clusters. Fleet offers an API, ClusterResourcePlacement, which helps orchestrate workloads, i.e., any group Kubernetes resources, among all member clusters. In this last part of the tutorial, you will use this API to place some Kubernetes resources automatically into the member clusters via the hub cluster, saving the trouble of having to create them one by one in each member cluster.\nCreate the resources for placement Run the commands below to create a namespace and a config map, which will be placed onto the member clusters.\nkubectl create namespace work kubectl create configmap app -n work --from-literal=data=test It may take a few seconds for the commands to complete.\nCreate the ClusterResourcePlacement API object Next, create a ClusterResourcePlacement API object in the hub cluster:\nkubectl apply -f - \u003c\u003cEOF apiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - group: \"\" kind: Namespace version: v1 name: work policy: placementType: PickAll EOF Note that the CRP object features a resource selector, which targets the work namespace you just created. This will instruct the CRP to place the namespace itself, and all resources registered under the namespace, such as the config map, to the target clusters. Also, in the policy field, a PickAll placement type has been specified. This allows the CRP to automatically perform the placement on all member clusters in the fleet, including those that join after the CRP object is created.\nIt may take a few seconds for Fleet to successfully place the resources. To check up on the progress, run the commands below:\nkubectl get clusterresourceplacement crp Verify that the placement has been completed successfully; you should see that the APPLIED status field has been set to True. You may need to repeat the commands a few times to wait for the completion.\nConfirm the placement Now, log into the member clusters to confirm that the placement has been completed.\nkubectl config use-context $MEMBER_CLUSTER_CONTEXT kubectl get ns kubectl get configmap -n work You should see the namespace work and the config map app listed in the output.\nClean things up To remove all the resources you just created, run the commands below:\n# This would also remove the namespace and config map placed in all member clusters. kubectl delete crp crp kubectl delete ns work kubectl delete configmap app -n work To uninstall Fleet, run the commands below:\nkubectl config use-context $HUB_CLUSTER_CONTEXT helm uninstall hub-agent kubectl config use-context $MEMBER_CLUSTER_CONTEXT helm uninstall member-agent What’s next Congratulations! You have completed the getting started tutorial for Fleet. To learn more about Fleet:\nRead about Fleet concepts Read about the ClusterResourcePlacement API Read the Fleet API reference ","categories":"","description":"Use on-premises clusters of your own to learn about Fleet","excerpt":"Use on-premises clusters of your own to learn about Fleet","ref":"/website/docs/getting-started/on-prem/","tags":"","title":"Getting started with Fleet using on-premises clusters"},{"body":" Fleet documentation features a number of how-to guides to help you complete common Fleet tasks. Pick one below to proceed.\n","categories":"","description":"Guides for completing common Fleet tasks","excerpt":"Guides for completing common Fleet tasks","ref":"/website/docs/how-tos/","tags":"","title":"How-To Guides"},{"body":"This how-to guide discusses how to use affinity settings to fine-tune how Fleet picks clusters for resource placement.\nAffinities terms are featured in the ClusterResourcePlacement API, specifically the scheduling policy section. Each affinity term is a particular requirement that Fleet will check against clusters; and the fulfillment of this requirement (or the lack of) would have certain effect on whether Fleet would pick a cluster for resource placement.\nFleet currently supports two types of affinity terms:\nrequiredDuringSchedulingIgnoredDuringExecution affinity terms; and perferredDuringSchedulingIgnoredDuringExecution affinity terms Most affinity terms deal with cluster labels. To manage member clusters, specifically adding/removing labels from a member cluster, see Managing Member Clusters How-To Guide.\nrequiredDuringSchedulingIgnoredDuringExecution affinity terms The requiredDuringSchedulingIgnoredDuringExecution type of affinity terms serves as a hard constraint that a cluster must satisfy before it can be picked. Each term may feature:\na label selector, which specifies a set of labels that a cluster must have or not have before it can be picked; a property selector, which specifies a cluster property requirement that a cluster must satisfy before it can be picked; a combination of both. For the specifics about property selectors, see the How-To Guide: Using Property-Based Scheduling.\nmatchLabels The most straightforward way is to specify matchLabels in the label selector, as showcased below:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: system: critical The example above includes a requiredDuringSchedulingIgnoredDuringExecution term which requires that the label system=critical must be present on a cluster before Fleet can pick it for the ClusterResourcePlacement.\nYou can add multiple labels to matchLabels; any cluster that satisfy this affinity term would have all the labels present.\nmatchExpressions For more complex logic, consider using matchExpressions, which allow you to use operators to set rules for validating labels on a member cluster. Each matchExpressions requirement includes:\na key, which is the key of the label; and\na list of values, which are the possible values for the label key; and\nan operator, which represents the relationship between the key and the list of values.\nSupported operators include:\nIn: the cluster must have a label key with one of the listed values. NotIn: the cluster must have a label key that is not associated with any of the listed values. Exists: the cluster must have the label key present; any value is acceptable. NotExists: the cluster must not have the label key. If you plan to use Exists and/or NotExists, you must leave the list of values empty.\nBelow is an example of matchExpressions affinity term using the In operator:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchExpressions: - key: system operator: In values: - critical - standard Any cluster with the label system=critical or system=standard will be picked by Fleet.\nSimilarly, you can also specify multiple matchExpressions requirements; any cluster that satisfy this affinity term would meet all the requirements.\nUsing both matchLabels and matchExpressions in one affinity term You can specify both matchLabels and matchExpressions in one requiredDuringSchedulingIgnoredDuringExecution affinity term, as showcased below:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: region: east matchExpressions: - key: system operator: Exists With this affinity term, any cluster picked must:\nhave the label region=east present; have the label system present, any value would do. Using multiple affinity terms You can also specify multiple requiredDuringSchedulingIgnoredDuringExecution affinity terms, as showcased below; a cluster will be picked if it can satisfy any affinity term.\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: region: west - labelSelector: matchExpressions: - key: system operator: DoesNotExist With these two affinity terms, any cluster picked must:\nhave the label region=west present; or does not have the label system preferredDuringSchedulingIgnoredDuringExecution affinity terms The preferredDuringSchedulingIgnoredDuringExecution type of affinity terms serves as a soft constraint for clusters; any cluster that satisfy such terms would receive an affinity score, which Fleet uses to rank clusters when processing ClusterResourcePlacement with scheduling policy of the PickN placement type.\nEach term features:\na weight, between -100 and 100, which is the affinity score that Fleet would assign to a cluster if it satisfies this term; and a label selector, or a property sorter. Both are required for this type of affinity terms to function.\nThe label selector is of the same struct as the one used in requiredDuringSchedulingIgnoredDuringExecution type of affinity terms; see the documentation above for usage.\nFor the specifics about property sorters, see the How-To Guide: Using Property-Based Scheduling.\nBelow is an example with a preferredDuringSchedulingIgnoredDuringExecution affinity term:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 10 affinity: clusterAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 preference: labelSelector: matchLabels: region: west Any cluster with the region=west label would receive an affinity score of 20.\nUsing multiple affinity terms Similarly, you can use multiple preferredDuringSchedulingIgnoredDuringExection affinity terms, as showcased below:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 10 affinity: clusterAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 preference: labelSelector: matchLabels: region: west - weight: -20 preference: labelSelector: matchLabels: environment: prod Cluster will be validated against each affinity term individually; the affinity scores it receives will be summed up. For example:\nif a cluster has only the region=west label, it would receive an affinity score of 20; however if a cluster has both the region=west and environment=prod labels, it would receive an affinity score of 20 + (-20) = 0. Use both types of affinity terms You can, if necessary, add both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExection types of affinity terms. Fleet will first run all clusters against all the requiredDuringSchedulingIgnoredDuringExecution type of affinity terms, filter out any that does not meet the requirements, and then assign the rest with affinity scores per preferredDuringSchedulingIgnoredDuringExection type of affinity terms.\nBelow is an example with both types of affinity terms:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 10 affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchExpressions: - key: system operator: Exists preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 preference: labelSelector: matchLabels: region: west With these affinity terms, only clusters with the label system (any value would do) can be picked; and among them, those with the region=west will be prioritized for resource placement as they receive an affinity score of 20.\n","categories":"","description":"How to use affinity settings in the `ClusterResourcePlacement` API to fine-tune Fleet scheduling decisions","excerpt":"How to use affinity settings in the `ClusterResourcePlacement` API to …","ref":"/website/docs/how-tos/affinities/","tags":"","title":"Using Affinity to Pick Clusters"},{"body":"Overview The fleet constitutes an implementation of a ClusterSet and encompasses the following attributes:\nA collective of clusters managed by a centralized authority. Typically characterized by a high level of mutual trust within the cluster set. Embraces the principle of Namespace Sameness across clusters: Ensures uniform permissions and characteristics for a given namespace across all clusters. While not mandatory for every cluster, namespaces exhibit consistent behavior across those where they are present. The MemberCluster represents a cluster-scoped API established within the hub cluster, serving as a representation of a cluster within the fleet. This API offers a dependable, uniform, and automated approach for multi-cluster applications (frameworks, toolsets) to identify registered clusters within a fleet. Additionally, it facilitates applications in querying a list of clusters managed by the fleet or observing cluster statuses for subsequent actions.\nSome illustrative use cases encompass:\nThe Fleet Scheduler utilizing managed cluster statuses or specific cluster properties (e.g., labels, taints) of a MemberCluster for resource scheduling. Automation tools like GitOps systems (e.g., ArgoCD or Flux) automatically registering/deregistering clusters in compliance with the MemberCluster API. The MCS API automatically generating ServiceImport CRs based on the MemberCluster CR defined within a fleet. Moreover, it furnishes a user-friendly interface for human operators to monitor the managed clusters.\nMemberCluster Lifecycle Joining the Fleet The process to join the Fleet involves creating a MemberCluster. The MemberCluster controller, a constituent of the hub-cluster-agent described in the Component, watches the MemberCluster CR and generates a corresponding namespace for the member cluster within the hub cluster. It configures roles and role bindings within the hub cluster, authorizing the specified member cluster identity (as detailed in the MemberCluster spec) access solely to resources within that namespace. To collate member cluster status, the controller generates another internal CR named InternalMemberCluster within the newly formed namespace. Simultaneously, the InternalMemberCluster controller, a component of the member-cluster-agent situated in the member cluster, gathers statistics on cluster usage, such as capacity utilization, and reports its status based on the HeartbeatPeriodSeconds specified in the CR. Meanwhile, the MemberCluster controller consolidates agent statuses and marks the cluster as Joined.\nLeaving the Fleet Fleet administrators can deregister a cluster by deleting the MemberCluster CR. Upon detection of deletion events by the MemberCluster controller within the hub cluster, it removes the corresponding InternalMemberCluster CR in the reserved namespace of the member cluster. It awaits completion of the “leave” process by the InternalMemberCluster controller of member agents, and then deletes role and role bindings and other resources including the member cluster reserved namespaces on the hub cluster.\nTaints Taints are a mechanism to prevent the Fleet Scheduler from scheduling resources to a MemberCluster. We adopt the concept of taints and tolerations introduced in Kubernetes to the multi-cluster use case.\nThe MemberCluster CR supports the specification of list of taints, which are applied to the MemberCluster. Each Taint object comprises the following fields:\nkey: The key of the taint. value: The value of the taint. effect: The effect of the taint, which can be NoSchedule for now. Once a MemberCluster is tainted with a specific taint, it lets the Fleet Scheduler know that the MemberCluster should not receive resources as part of the workload propagation from the hub cluster.\nThe NoSchedule taint is a signal to the Fleet Scheduler to avoid scheduling resources from a ClusterResourcePlacement to the MemberCluster. Any MemberCluster already selected for resource propagation will continue to receive resources even if a new taint is added.\nTaints are only honored by ClusterResourcePlacement with PickAll, PickN placement policies. In the case of PickFixed placement policy the taints are ignored because the user has explicitly specify the MemberClusters where the resources should be placed.\nFor detailed instructions, please refer to this document.\nWhat’s next Get hands-on experience how to add a member cluster to a fleet. Explore the ClusterResourcePlacement concept to placement cluster scope resources among managed clusters. ","categories":"","description":"Concept about the MemberCluster API","excerpt":"Concept about the MemberCluster API","ref":"/website/docs/concepts/membercluster/","tags":"","title":"MemberCluster"},{"body":"This how-to guide discusses how to use topology spread constraints to fine-tune how Fleet picks clusters for resource placement.\nTopology spread constraints are features in the ClusterResourcePlacement API, specifically the scheduling policy section. Generally speaking, these constraints can help you spread resources evenly across different groups of clusters in your fleet; or in other words, it assures that Fleet will not pick too many clusters from one group, and too little from another. You can use topology spread constraints to, for example:\nachieve high-availability for your database backend by making sure that there is at least one database replica in each region; or verify if your application can support clusters of different configurations; or eliminate resource utilization hotspots in your infrastructure through spreading jobs evenly across sections. Specifying a topology spread constraint A topology spread constraint consists of three fields:\ntopologyKey is a label key which Fleet uses to split your clusters from a fleet into different groups.\nSpecifically, clusters are grouped by the label values they have. For example, if you have three clusters in a fleet:\ncluster bravelion with the label system=critical and region=east; and cluster smartfish with the label system=critical and region=west; and cluster jumpingcat with the label system=normal and region=east, and you use system as the topology key, the clusters will be split into 2 groups:\ngroup 1 with cluster bravelion and smartfish, as they both have the value critical for label system; and group 2 with cluster jumpingcat, as it has the value normal for label system. Note that the splitting concerns only one label system; other labels, such as region, do not count.\nIf a cluster does not have the given topology key, it does not belong to any group. Fleet may still pick this cluster, as placing resources on it does not violate the associated topology spread constraint.\nThis is a required field.\nmaxSkew specifies how unevenly resource placements are spread in your fleet.\nThe skew of a set of resource placements are defined as the difference in count of resource placements between the group with the most and the group with the least, as split by the topology key.\nFor example, in the fleet described above (3 clusters, 2 groups):\nif Fleet picks two clusters from group A, but none from group B, the skew would be 2 - 0 = 2; however, if Fleet picks one cluster from group A and one from group B, the skew would be 1 - 1 = 0. The minimum value of maxSkew is 1. The less you set this value with, the more evenly resource placements are spread in your fleet.\nThis is a required field.\nNote\nNaturally, maxSkew only makes sense when there are no less than two groups. If you set a topology key that will not split the Fleet at all (i.e., all clusters with the given topology key has exactly the same value), the associated topology spread constraint will take no effect.\nwhenUnsatisfiable specifies what Fleet would do when it exhausts all options to satisfy the topology spread constraint; that is, picking any cluster in the fleet would lead to a violation.\nTwo options are available:\nDoNotSchedule: with this option, Fleet would guarantee that the topology spread constraint will be enforced all time; scheduling may fail if there is simply no possible way to satisfy the topology spread constraint.\nScheduleAnyway: with this option, Fleet would enforce the topology spread constraint in a best-effort manner; Fleet may, however, pick clusters that would violate the topology spread constraint if there is no better option.\nThis is an optional field; if you do not specify a value, Fleet will use DoNotSchedule by default.\nBelow is an example of topology spread constraint, which tells Fleet to pick clusters evenly from different groups, split based on the value of the label system:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 3 topologySpreadConstraints: - maxSkew: 2 topologyKey: system whenUnsatisfiable: DoNotSchedule How Fleet enforces topology spread constraints: topology spread scores When you specify some topology spread constraints in the scheduling policy of a ClusterResourcePlacement object, Fleet will start picking clusters one at a time. More specifically, Fleet will:\nfor each cluster in the fleet, evaluate how skew would change if resources were placed on it.\nDepending on the current spread of resource placements, there are three possible outcomes:\nplacing resources on the cluster reduces the skew by 1; or placing resources on the cluster has no effect on the skew; or placing resources on the cluster increases the skew by 1. Fleet would then assign a topology spread score to the cluster:\nif the provisional placement reduces the skew by 1, the cluster receives a topology spread score of 1; or\nif the provisional placement has no effect on the skew, the cluster receives a topology spread score of 0; or\nif the provisional placement increases the skew by 1, but does not yet exceed the max skew specified in the constraint, the cluster receives a topology spread score of -1; or\nif the provisional placement increases the skew by 1, and has exceeded the max skew specified in the constraint,\nfor topology spread constraints with the ScheduleAnyway effect, the cluster receives a topology spread score of -1000; and for those with the DoNotSchedule effect, the cluster will be removed from resource placement consideration. rank the clusters based on the topology spread score and other factors (e.g., affinity), pick the one that is most appropriate.\nrepeat the process, until all the needed count of clusters are found.\nBelow is an example that illustrates the process:\nSuppose you have a fleet of 4 clusters:\ncluster bravelion, with label region=east and system=critical; and cluster smartfish, with label region=east; and cluster jumpingcat, with label region=west, and system=critical; and cluster flyingpenguin, with label region=west, And you have created a ClusterResourcePlacement as follows:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 2 topologySpreadConstraints: - maxSkew: 1 topologyKey: region whenUnsatisfiable: DoNotSchedule Fleet will first scan all the 4 clusters in the fleet; they all have the region label, with two different values east and west (2 cluster in each of them). This divides the clusters into two groups, the east and the west\nAt this stage, no cluster has been picked yet, so there is no resource placement at all. The current skew is thus 0, and placing resources on any of them would increase the skew by 1. This is still below the maxSkew threshold given, so all clusters would receive a topology spread score of -1.\nFleet could not find the most appropriate cluster based on the topology spread score so far, so it would resort to other measures for ranking clusters. This would lead Fleet to pick cluster smartfish.\nNote\nSee Using ClusterResourcePlacement to Place Resources How-To Guide for more information on how Fleet picks clusters.\nNow, one cluster has been picked, and one more is needed by the ClusterResourcePlacement object (as the numberOfClusters field is set to 2). Fleet scans the left 3 clusters again, and this time, since smartfish from group east has been picked, any more resource placement on clusters from group east would increase the skew by 1 more, and would lead to violation of the topology spread constraint; Fleet will then assign the topology spread score of -1000 to cluster bravelion, which is in group east. On the contrary, picking a cluster from any cluster in group west would reduce the skew by 1, so Fleet assigns the topology spread score of 1 to cluster jumpingcat and flyingpenguin.\nWith the higher topology spread score, jumpingcat and flyingpenguin become the leading candidate in ranking. They have the same topology spread score, and based on the rules Fleet has for picking clusters, jumpingcat would be picked finally.\nUsing multiple topology spread constraints You can, if necessary, use multiple topology spread constraints. Fleet will evaluate each of them separately, and add up topology spread scores for each cluster for the final ranking. A cluster would be removed from resource placement consideration if placing resources on it would violate any one of the DoNotSchedule topology spread constraints.\nBelow is an example where two topology spread constraints are used:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 2 topologySpreadConstraints: - maxSkew: 2 topologyKey: region whenUnsatisfiable: DoNotSchedule - maxSkew: 3 topologyKey: environment whenUnsatisfiable: ScheduleAnyway Note\nIt might be very difficult to find candidate clusters when multiple topology spread constraints are added. Considering using the ScheduleAnyway effect to add some leeway to the scheduling, if applicable.\n","categories":"","description":"How to use topology spread constraints in the `ClusterResourcePlacement` API to fine-tune Fleet scheduling decisions","excerpt":"How to use topology spread constraints in the …","ref":"/website/docs/how-tos/topology-spread-constraints/","tags":"","title":"Using Topology Spread Constraints to Spread Resources"},{"body":"What are fleet-owned resources on the hub and member clusters? Can these fleet-owned resources be modified by the user? Majority of the internal resources and fleet reserved namespaces described below are safeguarded by a series of validating webhooks, serving as a preventive measure to restrict users from making modifications to them.\nThe fleet reserved namespace are fleet-system and fleet-member-{clusterName} where clusterName is the name of each member cluster that has joined the fleet.\nFleet hub cluster internal resources:\nResource InternalMemberCluster Work ClusterResourceSnapshot ClusterSchedulingPolicySnapshot ClusterResourceBinding Fleet member cluster internal resources:\nResource InternalMemberCluster AppliedWork Fleet APIs are defined here, Fleet CRDs are defined here.\nFleet Networking hub cluster internal resources: Resource EndpointSliceExport EndpointSliceImport InternalServiceExport InternalServiceImport ServiceImport Fleet Networking APIs are defined here, Fleet Networking CRDs are defined here.\nWhat kind of the resources are allowed to be propagated from the hub cluster to the member clusters? How can I control the list? The resources to be propagated from the hub cluster to the member clusters can be controlled by either an exclude/skip list or an include/allow list which are mutually exclusive.\nClusterResourcePlacement excludes certain groups/resources when propagating the resources by default. They are defined here.\nk8s.io/api/events/v1 (group) k8s.io/api/coordination/v1 (group) k8s.io/metrics/pkg/apis/metrics/v1beta1 (group) k8s.io/api/core/v1 (pod, node) networking.fleet.azure.com (service import resource) any resources in the “default” namespace You can use skipped-propagating-apis and skipped-propagating-namespaces flag when installing the hub-agent to skip resources from being propagated by specifying their group/group-version/group-version-kind and namespaces.\nYou can use allowed-propagating-apis flag on the hub-agent to only allow propagation of desired set of resources specified in the form of group/group-version/group-version-kind. This flag is mutually exclusive with skipped-propagating-apis.\nWhat happens to existing resources in member clusters when their definitions conflict with the desired resources in the hub cluster? In case of a conflict, where a resource already exists on the member cluster, the apply operation fails when trying to propagate the same resource from the hub cluster.\nWhat happens if modifies resources that were placed from hub to member clusters? Possible scenarios:\nIf the user updates the resource on the hub cluster, the update is propagated to all member clusters where the resource exists. If the user deletes the resource on the hub cluster, the resource is deleted on all clusters to which it was propagated. If the user updates the resource on the member cluster, no automatic action occurs as it’s a user-made modification. If the user deletes the resource on the member cluster, the resource is automatically created again on the member cluster after reconciliation. ","categories":"","description":"Frequently Asked Questions about Fleet","excerpt":"Frequently Asked Questions about Fleet","ref":"/website/docs/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"The scheduler component is a vital element in Fleet workload scheduling. Its primary responsibility is to determine the schedule decision for a bundle of resources based on the latest ClusterSchedulingPolicySnapshotgenerated by the ClusterResourcePlacement. By default, the scheduler operates in batch mode, which enhances performance. In this mode, it binds a ClusterResourceBinding from a ClusterResourcePlacement to multiple clusters whenever possible.\nBatch in nature Scheduling resources within a ClusterResourcePlacement involves more dependencies compared with scheduling pods within a deployment in Kubernetes. There are two notable distinctions:\nIn a ClusterResourcePlacement, multiple replicas of resources cannot be scheduled on the same cluster, whereas pods belonging to the same deployment in Kubernetes can run on the same node. The ClusterResourcePlacement supports different placement types within a single object. These requirements necessitate treating the scheduling policy as a whole and feeding it to the scheduler, as opposed to handling individual pods like Kubernetes today. Specially:\nScheduling the entire ClusterResourcePlacement at once enables us to increase the parallelism of the scheduler if needed. Supporting the PickAll mode would require generating the replica for each cluster in the fleet to scheduler. This approach is not only inefficient but can also result in scheduler repeatedly attempting to schedule unassigned replica when there are no possibilities of placing them. To support the PickN mode, the scheduler needs to compute the filtering and scoring for each replica. Conversely, in batch mode, these calculations are performed once. Scheduler sorts all the eligible clusters and pick the top N clusters. Placement Decisions The output of the scheduler is an array of ClusterResourceBindings on the hub cluster.\nClusterResourceBinding sample:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourceBinding metadata: annotations: kubernetes-fleet.io/previous-binding-state: Bound creationTimestamp: \"2023-11-06T09:53:11Z\" finalizers: - kubernetes-fleet.io/work-cleanup generation: 8 labels: kubernetes-fleet.io/parent-CRP: crp-1 name: crp-1-aks-member-1-2f8fe606 resourceVersion: \"1641949\" uid: 3a443dec-a5ad-4c15-9c6d-05727b9e1d15 spec: clusterDecision: clusterName: aks-member-1 clusterScore: affinityScore: 0 priorityScore: 0 reason: picked by scheduling policy selected: true resourceSnapshotName: crp-1-4-snapshot schedulingPolicySnapshotName: crp-1-1 state: Bound targetCluster: aks-member-1 status: conditions: - lastTransitionTime: \"2023-11-06T09:53:11Z\" message: \"\" observedGeneration: 8 reason: AllWorkSynced status: \"True\" type: Bound - lastTransitionTime: \"2023-11-10T08:23:38Z\" message: \"\" observedGeneration: 8 reason: AllWorkHasBeenApplied status: \"True\" type: Applied ClusterResourceBinding can have three states:\nScheduled: It indicates that the scheduler has selected this cluster for placing the resources. The resource is waiting to be picked up by the rollout controller. Bound: It indicates that the rollout controller has initiated the placement of resources on the target cluster. The resources are actively being deployed. Unscheduled: This states signifies that the target cluster is no longer selected by the scheduler for the placement. The resource associated with this cluster are in the process of being removed. They are awaiting deletion from the cluster. The scheduler operates by generating scheduling decisions through the creating of new bindings in the “scheduled” state and the removal of existing bindings by marking them as “unscheduled”. There is a separate rollout controller which is responsible for executing these decisions based on the defined rollout strategy.\nEnforcing the semantics of “IgnoreDuringExecutionTime” The ClusterResourcePlacement enforces the semantics of “IgnoreDuringExecutionTime” to prioritize the stability of resources running in production. Therefore, the resources should not be moved or rescheduled without explicit changes to the scheduling policy.\nHere are some high-level guidelines outlining the actions that trigger scheduling and corresponding behavior:\nPolicy changes trigger scheduling:\nThe scheduler makes the placement decisions based on the latest ClusterSchedulingPolicySnapshot. When it’s just a scale out operation (NumberOfClusters of pickN mode is increased), the ClusterResourcePlacement controller updates the label of the existing ClusterSchedulingPolicySnapshot instead of creating a new one, so that the scheduler won’t move any existing resources that are already scheduled and just fulfill the new requirement. The following cluster changes trigger scheduling:\na cluster, originally ineligible for resource placement for some reason, becomes eligible, such as: the cluster setting changes, specifically MemberCluster labels has changed an unexpected deployment which originally leads the scheduler to discard the cluster (for example, agents not joining, networking issues, etc.) has been resolved a cluster, originally eligible for resource placement, is leaving the fleet and becomes ineligible Note: The scheduler is only going to place the resources on the new cluster and won’t touch the existing clusters.\nResource-only changes do not trigger scheduling including:\nResourceSelectors is updated in the ClusterResourcePlacement spec. The selected resources is updated without directly affecting the ClusterResourcePlacement. What’s next Read about Scheduling Framework ","categories":"","description":"Concept about the Fleet scheduler","excerpt":"Concept about the Fleet scheduler","ref":"/website/docs/concepts/scheduler/","tags":"","title":"Scheduler"},{"body":"This how-to guide discusses how to use property-based scheduling to produce scheduling decisions based on cluster properties.\nNote\nThe availability of properties depend on which (and if) you have a property provider set up in your Fleet deployment. For more information, see the Concept: Property Provider and Cluster Properties documentation.\nIt is also recommended that you read the How-To Guide: Using Affinity to Pick Clusters first before following instructions in this document.\nFleet allows users to pick clusters based on exposed cluster properties via the affinity terms in the ClusterResourcePlacement API:\nfor the requiredDuringSchedulingIgnoredDuringExecution affinity terms, you may specify property selectors to filter clusters based on their properties; for the preferredDuringSchedulingIgnoredDuringExecution affinity terms, you may specify property sorters to prefer clusters with a property that ranks higher or lower. Property selectors in requiredDuringSchedulingIgnoredDuringExecution affinity terms A property selector is an array of expression matchers against cluster properties. In each matcher you will specify:\nA name, which is the name of the property.\nIf the property is a non-resource one, you may refer to it directly here; however, if the property is a resource one, the name here should be of the following format:\nresources.kubernetes-fleet.io/[CAPACITY-TYPE]-[RESOURCE-NAME] where [CAPACITY-TYPE] is one of total, allocatable, or available, depending on which capacity (usage information) you would like to check against, and [RESOURCE-NAME] is the name of the resource.\nFor example, if you would like to select clusters based on the available CPU capacity of a cluster, the name used in the property selector should be\nresources.kubernetes-fleet.io/available-cpu and for the allocatable memory capacity, use\nresources.kubernetes-fleet.io/allocatable-memory A list of values, which are possible values of the property.\nAn operator, which describes the relationship between a cluster’s observed value of the given property and the list of values in the matcher.\nCurrently, available operators are\nGt (Greater than): a cluster’s observed value of the given property must be greater than the value in the matcher before it can be picked for resource placement. Ge (Greater than or equal to): a cluster’s observed value of the given property must be greater than or equal to the value in the matcher before it can be picked for resource placement. Lt (Less than): a cluster’s observed value of the given property must be less than the value in the matcher before it can be picked for resource placement. Le (Less than or equal to): a cluster’s observed value of the given property must be less than or equal to the value in the matcher before it can be picked for resource placement. Eq (Equal to): a cluster’s observed value of the given property must be equal to the value in the matcher before it can be picked for resource placement. Ne (Not equal to): a cluster’s observed value of the given property must be not equal to the value in the matcher before it can be picked for resource placement. Note that if you use the operator Gt, Ge, Lt, Le, Eq, or Ne, the list of values in the matcher should have exactly one value.\nFleet will evaluate each cluster, specifically their exposed properties, against the matchers; failure to satisfy any matcher in the selector will exclude the cluster from resource placement.\nNote that if a cluster does not have the specified property for a matcher, it will automatically fail the matcher.\nBelow is an example that uses a property selector to select only clusters with a node count of at least 5 for resource placement:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - propertySelector: matchExpressions: - name: \"kubernetes-fleet.io/node-count\" operator: Ge values: - \"5\" You may use both label selector and property selector in a requiredDuringSchedulingIgnoredDuringExecution affinity term. Both selectors must be satisfied before a cluster can be picked for resource placement:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: region: east propertySelector: matchExpressions: - name: \"kubernetes-fleet.io/node-count\" operator: Ge values: - \"5\" In the example above, Fleet will only consider a cluster for resource placement if it has the region=east label and a node count no less than 5.\nProperty sorters in preferredDuringSchedulingIgnoredDuringExecution affinity terms A property sorter ranks all the clusters in the Fleet based on their values of a specified property in ascending or descending order, then yields weights for the clusters in proportion to their ranks. The proportional weights are calculated based on the weight value given in the preferredDuringSchedulingIgnoredDuringExecution term.\nA property sorter consists of:\nA name, which is the name of the property; see the format in the previous section for more information.\nA sort order, which is one of Ascending and Descending, for ranking in ascending and descending order respectively.\nAs a rule of thumb, when the Ascending order is used, Fleet will prefer clusters with lower observed values, and when the Descending order is used, clusters with higher observed values will be preferred.\nWhen using the sort order Descending, the proportional weight is calculated using the formula:\n((Observed Value - Minimum observed value) / (Maximum observed value - Minimum observed value)) * Weight For example, suppose that you would like to rank clusters based on the property of available CPU capacity in descending order and currently, you have a fleet of 3 clusters with the available CPU capacities as follows:\nCluster Available CPU capacity bravelion 100 smartfish 20 jumpingcat 10 The sorter would yield the weights below:\nCluster Available CPU capacity Weight bravelion 100 (100 - 10) / (100 - 10) = 100% of the weight smartfish 20 (20 - 10) / (100 - 10) = 11.11% of the weight jumpingcat 10 (10 - 10) / (100 - 10) = 0% of the weight And when using the sort order Ascending, the proportional weight is calculated using the formula:\n(1 - ((Observed Value - Minimum observed value) / (Maximum observed value - Minimum observed value))) * Weight For example, suppose that you would like to rank clusters based on their per CPU core cost in ascending order and currently across the fleet, you have a fleet of 3 clusters with the per CPU core costs as follows:\nCluster Per CPU core cost bravelion 1 smartfish 0.2 jumpingcat 0.1 The sorter would yield the weights below:\nCluster Per CPU core cost Weight bravelion 1 1 - ((1 - 0.1) / (1 - 0.1)) = 0% of the weight smartfish 0.2 1 - ((0.2 - 0.1) / (1 - 0.1)) = 88.89% of the weight jumpingcat 0.1 1 - (0.1 - 0.1) / (1 - 0.1) = 100% of the weight The example below showcases a property sorter using the Descending order:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 10 affinity: clusterAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 preference: metricSorter: name: kubernetes-fleet.io/node-count sortOrder: Descending In this example, Fleet will prefer clusters with higher node counts. The cluster with the highest node count would receive a weight of 20, and the cluster with the lowest would receive 0. Other clusters receive proportional weights calculated using the formulas above.\nYou may use both label selector and property sorter in a preferredDuringSchedulingIgnoredDuringExecution affinity term. A cluster that fails the label selector would receive no weight, and clusters that pass the label selector receive proportional weights under the property sorter.\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp spec: resourceSelectors: - ... policy: placementType: PickN numberOfClusters: 10 affinity: clusterAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 preference: labelSelector: matchLabels: env: prod metricSorter: name: resources.kubernetes-fleet.io/total-cpu sortOrder: Descending In the example above, a cluster would only receive additional weight if it has the label env=prod, and the more total CPU capacity it has, the more weight it will receive, up to the limit of 20.\n","categories":"","description":"How to use property-based scheduling to produce scheduling decisions","excerpt":"How to use property-based scheduling to produce scheduling decisions","ref":"/website/docs/how-tos/property-based-scheduling/","tags":"","title":"Using Property-Based Scheduling"},{"body":"The fleet scheduling framework closely aligns with the native Kubernetes scheduling framework, incorporating several modifications and tailored functionalities.\nThe primary advantage of this framework lies in its capability to compile plugins directly into the scheduler. Its API facilitates the implementation of diverse scheduling features as plugins, thereby ensuring a lightweight and maintainable core.\nThe fleet scheduler integrates three fundamental built-in plugin types:\nTopology Spread Plugin: Supports the TopologySpreadConstraints stipulated in the placement policy. Cluster Affinity Plugin: Facilitates the Affinity clause of the placement policy. Same Placement Affinity Plugin: Uniquely designed for the fleet, preventing multiple replicas (selected resources) from being placed within the same cluster. This distinguishes it from Kubernetes, which allows multiple pods on a node. Cluster Eligibility Plugin: Enables cluster selection based on specific status criteria. ** Taint \u0026 Toleration Plugin**: Enables cluster selection based on taints on the cluster \u0026 tolerations on the ClusterResourcePlacement. Compared to the Kubernetes scheduling framework, the fleet framework introduces additional stages for the pickN placement type:\nBatch \u0026 PostBatch: Batch: Defines the batch size based on the desired and current ClusterResourceBinding. PostBatch: Adjusts the batch size as necessary. Unlike the Kubernetes scheduler, which schedules pods individually (batch size = 1). Sort: Fleet’s sorting mechanism selects a number of clusters, whereas Kubernetes’ scheduler prioritizes nodes with the highest scores. To streamline the scheduling framework, certain stages, such as permit and reserve, have been omitted due to the absence of corresponding plugins or APIs enabling customers to reserve or permit clusters for specific placements. However, the framework remains designed for easy extension in the future to accommodate these functionalities.\nIn-tree plugins The scheduler includes default plugins, each associated with distinct extension points:\nPlugin PostBatch Filter Score Cluster Affinity ❌ ✅ ✅ Same Placement Anti-affinity ❌ ✅ ❌ Topology Spread Constraints ✅ ✅ ✅ Cluster Eligibility ❌ ✅ ❌ Taint \u0026 Toleration ❌ ✅ ❌ The Cluster Affinity Plugin serves as an illustrative example and operates within the following extension points:\nPreFilter: Verifies whether the policy contains any required cluster affinity terms. If absent, the plugin bypasses the subsequent Filter stage. Filter: Filters out clusters that fail to meet the specified required cluster affinity terms outlined in the policy. PreScore: Determines if the policy includes any preferred cluster affinity terms. If none are found, this plugin will be skipped during the Score stage. Score: Assigns affinity scores to clusters based on compliance with the preferred cluster affinity terms stipulated in the policy. ","categories":"","description":"Concept about the Fleet scheduling framework","excerpt":"Concept about the Fleet scheduling framework","ref":"/website/docs/concepts/scheduling-framework/","tags":"","title":"Scheduling Framework"},{"body":"This how-to guide discusses how to add/remove taints on MemberCluster and how to add tolerations on ClusterResourcePlacement.\nAdding taint to MemberCluster In this example, we will add a taint to a MemberCluster. Then try to propagate resources to the MemberCluster using a ClusterResourcePlacement with PickAll placement policy. The resources should not be propagated to the MemberCluster because of the taint.\nWe will first create a namespace that we will propagate to the member cluster,\nkubectl create ns test-ns Then apply the MemberCluster with a taint,\nExample MemberCluster with taint:\napiVersion: cluster.kubernetes-fleet.io/v1beta1 kind: MemberCluster metadata: name: kind-cluster-1 spec: identity: name: fleet-member-agent-cluster-1 kind: ServiceAccount namespace: fleet-system apiGroup: \"\" taints: - key: test-key1 value: test-value1 effect: NoSchedule After applying the above MemberCluster, we will apply a ClusterResourcePlacement with the following spec:\nresourceSelectors: - group: \"\" kind: Namespace version: v1 name: test-ns policy: placementType: PickAll The ClusterResourcePlacement CR should not propagate the test-ns namespace to the member cluster because of the taint, looking at the status of the CR should show the following:\nstatus: conditions: - lastTransitionTime: \"2024-04-16T19:03:17Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 2 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2024-04-16T19:03:17Z\" message: All 0 cluster(s) are synchronized to the latest resources on the hub cluster observedGeneration: 2 reason: SynchronizeSucceeded status: \"True\" type: ClusterResourcePlacementSynchronized - lastTransitionTime: \"2024-04-16T19:03:17Z\" message: There are no clusters selected to place the resources observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: ClusterResourcePlacementApplied observedResourceIndex: \"0\" selectedResources: - kind: Namespace name: test-ns version: v1 Looking at the ClusterResourcePlacementSynchronized, ClusterResourcePlacementApplied conditions and reading the message fields we can see that no clusters were selected to place the resources.\nRemoving taint from MemberCluster In this example, we will remove the taint from the MemberCluster from the last section. This should automatically trigger the Fleet scheduler to propagate the resources to the MemberCluster.\nAfter removing the taint from the MemberCluster. Let’s take a look at the status of the ClusterResourcePlacement:\nstatus: conditions: - lastTransitionTime: \"2024-04-16T20:00:03Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 2 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2024-04-16T20:02:57Z\" message: All 1 cluster(s) are synchronized to the latest resources on the hub cluster observedGeneration: 2 reason: SynchronizeSucceeded status: \"True\" type: ClusterResourcePlacementSynchronized - lastTransitionTime: \"2024-04-16T20:02:57Z\" message: Successfully applied resources to 1 member clusters observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: ClusterResourcePlacementApplied observedResourceIndex: \"0\" placementStatuses: - clusterName: kind-cluster-1 conditions: - lastTransitionTime: \"2024-04-16T20:02:52Z\" message: 'Successfully scheduled resources for placement in kind-cluster-1 (affinity score: 0, topology spread score: 0): picked by scheduling policy' observedGeneration: 2 reason: ScheduleSucceeded status: \"True\" type: Scheduled - lastTransitionTime: \"2024-04-16T20:02:57Z\" message: Successfully Synchronized work(s) for placement observedGeneration: 2 reason: WorkSynchronizeSucceeded status: \"True\" type: WorkSynchronized - lastTransitionTime: \"2024-04-16T20:02:57Z\" message: Successfully applied resources observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: Applied selectedResources: - kind: Namespace name: test-ns version: v1 From the status we can clearly see that the resources were propagated to the member cluster after removing the taint.\nAdding toleration to ClusterResourcePlacement Adding a toleration to a ClusterResourcePlacement CR allows the Fleet scheduler to tolerate specific taints on the MemberClusters.\nFor this section we will start from scratch, we will first create a namespace that we will propagate to the MemberCluster\nkubectl create ns test-ns Then apply the MemberCluster with a taint,\nExample MemberCluster with taint:\nspec: heartbeatPeriodSeconds: 60 identity: apiGroup: \"\" kind: ServiceAccount name: fleet-member-agent-cluster-1 namespace: fleet-system taints: - effect: NoSchedule key: test-key1 value: test-value1 The ClusterResourcePlacement CR will not propagate the test-ns namespace to the member cluster because of the taint.\nNow we will add a toleration to a ClusterResourcePlacement CR as part of the placement policy, which will use the Exists operator to tolerate the taint.\nExample ClusterResourcePlacement spec with tolerations after adding new toleration:\nspec: policy: placementType: PickAll tolerations: - key: test-key1 operator: Exists resourceSelectors: - group: \"\" kind: Namespace name: test-ns version: v1 revisionHistoryLimit: 10 strategy: type: RollingUpdate Let’s take a look at the status of the ClusterResourcePlacement CR after adding the toleration:\nstatus: conditions: - lastTransitionTime: \"2024-04-16T20:16:10Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 3 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2024-04-16T20:16:15Z\" message: All 1 cluster(s) are synchronized to the latest resources on the hub cluster observedGeneration: 3 reason: SynchronizeSucceeded status: \"True\" type: ClusterResourcePlacementSynchronized - lastTransitionTime: \"2024-04-16T20:16:15Z\" message: Successfully applied resources to 1 member clusters observedGeneration: 3 reason: ApplySucceeded status: \"True\" type: ClusterResourcePlacementApplied observedResourceIndex: \"0\" placementStatuses: - clusterName: kind-cluster-1 conditions: - lastTransitionTime: \"2024-04-16T20:16:10Z\" message: 'Successfully scheduled resources for placement in kind-cluster-1 (affinity score: 0, topology spread score: 0): picked by scheduling policy' observedGeneration: 3 reason: ScheduleSucceeded status: \"True\" type: Scheduled - lastTransitionTime: \"2024-04-16T20:16:15Z\" message: Successfully Synchronized work(s) for placement observedGeneration: 3 reason: WorkSynchronizeSucceeded status: \"True\" type: WorkSynchronized - lastTransitionTime: \"2024-04-16T20:16:15Z\" message: Successfully applied resources observedGeneration: 3 reason: ApplySucceeded status: \"True\" type: Applied selectedResources: - kind: Namespace name: test-ns version: v1 From the status we can see that the resources were propagated to the MemberCluster after adding the toleration.\nNow let’s try adding a new taint to the member cluster CR and see if the resources are still propagated to the MemberCluster,\nExample MemberCluster CR with new taint:\nheartbeatPeriodSeconds: 60 identity: apiGroup: \"\" kind: ServiceAccount name: fleet-member-agent-cluster-1 namespace: fleet-system taints: - effect: NoSchedule key: test-key1 value: test-value1 - effect: NoSchedule key: test-key2 value: test-value2 Let’s take a look at the ClusterResourcePlacement CR status after adding the new taint:\nstatus: conditions: - lastTransitionTime: \"2024-04-16T20:27:44Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 2 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2024-04-16T20:27:49Z\" message: All 1 cluster(s) are synchronized to the latest resources on the hub cluster observedGeneration: 2 reason: SynchronizeSucceeded status: \"True\" type: ClusterResourcePlacementSynchronized - lastTransitionTime: \"2024-04-16T20:27:49Z\" message: Successfully applied resources to 1 member clusters observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: ClusterResourcePlacementApplied observedResourceIndex: \"0\" placementStatuses: - clusterName: kind-cluster-1 conditions: - lastTransitionTime: \"2024-04-16T20:27:44Z\" message: 'Successfully scheduled resources for placement in kind-cluster-1 (affinity score: 0, topology spread score: 0): picked by scheduling policy' observedGeneration: 2 reason: ScheduleSucceeded status: \"True\" type: Scheduled - lastTransitionTime: \"2024-04-16T20:27:49Z\" message: Successfully Synchronized work(s) for placement observedGeneration: 2 reason: WorkSynchronizeSucceeded status: \"True\" type: WorkSynchronized - lastTransitionTime: \"2024-04-16T20:27:49Z\" message: Successfully applied resources observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: Applied selectedResources: - kind: Namespace name: test-ns version: v1 Nothing changes in the status because even if the new taint is not tolerated, the exising resources on the MemberCluster will continue to run because the taint effect is NoSchedule and the cluster was already selected for resource propagation in a previous scheduling cycle.\n","categories":"","description":"How to use taints and tolerations to fine-tune scheduling decisions","excerpt":"How to use taints and tolerations to fine-tune scheduling decisions","ref":"/website/docs/how-tos/taints-tolerations/","tags":"","title":"Using Taints and Tolerations"},{"body":"This document explains the concepts of property provider and cluster properties in Fleet.\nFleet allows developers to implement a property provider to expose arbitrary properties about a member cluster, such as its node count and available resources for workload placement. Platforms could also enable their property providers to expose platform-specific properties via Fleet. These properties can be useful in a variety of cases: for example, administrators could monitor the health of a member cluster using related properties; Fleet also supports making scheduling decisions based on the property data.\nProperty provider A property provider implements Fleet’s property provider interface:\n// PropertyProvider is the interface that every property provider must implement. type PropertyProvider interface { // Collect is called periodically by the Fleet member agent to collect properties. // // Note that this call should complete promptly. Fleet member agent will cancel the // context if the call does not complete in time. Collect(ctx context.Context) PropertyCollectionResponse // Start is called when the Fleet member agent starts up to initialize the property provider. // This call should not block. // // Note that Fleet member agent will cancel the context when it exits. Start(ctx context.Context, config *rest.Config) error } For the details, see the Fleet source code.\nA property provider should be shipped as a part of the Fleet member agent and run alongside it. Refer to the Fleet source code for specifics on how to set it up with the Fleet member agent. At this moment, only one property provider can be set up with the Fleet member agent at a time. Once connected, the Fleet member agent will attempt to start it when the agent itself initializes; the agent will then start collecting properties from the property provider periodically.\nA property provider can expose two types of properties: resource properties, and non-resource properties. To learn about the two types, see the section below. In addition, the provider can choose to report its status, such as any errors encountered when preparing the properties, in the form of Kubernetes conditions.\nThe Fleet member agent can run with or without a property provider. If a provider is not set up, or the given provider fails to start properly, the agent will collect limited properties about the cluster on its own, specifically the node count, plus the total/allocatable CPU and memory capacities of the host member cluster.\nCluster properties A cluster property is an attribute of a member cluster. There are two types of properties:\nResource property: the usage information of a resource in a member cluster; the name of the resource should be in the format of a Kubernetes label key, such as cpu and memory, and the usage information should consist of:\nthe total capacity of the resource, which is the amount of the resource installed in the cluster; the allocatable capacity of the resource, which is the maximum amount of the resource that can be used for running user workloads, as some amount of the resource might be reserved by the OS, kubelet, etc.; the available capacity of the resource, which is the amount of the resource that is currently free for running user workloads. Note that you may report a virtual resource via the property provider, if applicable.\nNon-resource property: a metric about a member cluster, in the form of a key/value pair; the key should be in the format of a Kubernetes label key, such as kubernetes-fleet.io/node-count, and the value at this moment should be a sortable numeric that can be parsed as a Kubernetes quantity.\nEventually, all cluster properties are exposed via the Fleet MemberCluster API, with the non-resource properties in the .status.properties field and the resource properties .status.resourceUsage field:\napiVersion: cluster.kubernetes-fleet.io/v1beta1 kind: MemberCluster metadata: ... spec: ... status: agentStatus: ... conditions: ... properties: kubernetes-fleet.io/node-count: observationTime: \"2024-04-30T14:54:24Z\" value: \"2\" ... resourceUsage: allocatable: cpu: 32 memory: \"16Gi\" available: cpu: 2 memory: \"800Mi\" capacity: cpu: 40 memory: \"20Gi\" Note that conditions reported by the property provider (if any), would be available in the .status.conditions array as well.\nCore properties The following properties are considered core properties in Fleet, which should be supported in all property provider implementations. Fleet agents will collect them even when no property provider has been set up.\nProperty Type Name Description Non-resource property kubernetes-fleet.io/node-count The number of nodes in a cluster. Resource property cpu The usage information (total, allocatable, and available capacity) of CPU resource in a cluster. Resource property memory The usage information (total, allocatable, and available capacity) of memory resource in a cluster. ","categories":"","description":"Concept about cluster properties and property provides","excerpt":"Concept about cluster properties and property provides","ref":"/website/docs/concepts/properties/","tags":"","title":"Properties and Property Provides"},{"body":"This guide provides an overview of how to use the Fleet ClusterResourceOverride API to override cluster resources.\nOverview ClusterResourceOverride is a feature within Fleet that allows for the modification or override of specific attributes across cluster-wide resources. With ClusterResourceOverride, you can define rules based on cluster labels or other criteria, specifying changes to be applied to various cluster-wide resources such as namespaces, roles, role bindings, or custom resource definitions. These modifications may include updates to permissions, configurations, or other parameters, ensuring consistent management and enforcement of configurations across your Fleet-managed Kubernetes clusters.\nAPI Components The ClusterResourceOverride API consists of the following components:\nCluster Resource Selectors: These specify the set of cluster resources selected for overriding. Policy: This specifies the policy to be applied to the selected resources. The following sections discuss these components in depth.\nCluster Resource Selectors A ClusterResourceOverride object may feature one or more cluster resource selectors, specifying which resources to select to be overridden.\nThe ClusterResourceSelector object supports the following fields:\ngroup: The API group of the resource version: The API version of the resource kind: The kind of the resource name: The name of the resource Note: The resource can only be selected by name.\nTo add a resource selector, edit the clusterResourceSelectors field in the ClusterResourceOverride spec:\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ClusterResourceOverride metadata: name: example-cro spec: clusterResourceSelectors: - group: rbac.authorization.k8s.io kind: ClusterRole version: v1 name: secret-reader The example above will pick the ClusterRole named secret-reader, as shown below, to be overridden.\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] Policy The Policy is made up of a set of rules (OverrideRules) that specify the changes to be applied to the selected resources on selected clusters.\nEach OverrideRule supports the following fields:\nCluster Selector: This specifies the set of clusters to which the override applies. JSON Patch Override: This specifies the changes to be applied to the selected resources. To add an override rule, edit the policy field in the ClusterResourceOverride spec:\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ClusterResourceOverride metadata: name: example-cro spec: clusterResourceSelectors: - group: rbac.authorization.k8s.io kind: ClusterRole version: v1 name: secret-reader policy: overrideRules: - clusterSelector: clusterSelectorTerms: - labelSelector: matchLabels: env: prod jsonPatchOverrides: - op: remove path: /rules/0/verbs/2 The ClusterResourceOverride object above will remove the verb “list” in the ClusterRole named secret-reader on clusters with the label env: prod.\nThe ClusterResourceOverride mentioned above utilizes the cluster role displayed below:\nName: secret-reader Labels: \u003cnone\u003e Annotations: \u003cnone\u003e PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- secrets [] [] [get watch list] Cluster Selector To specify the clusters to which the override applies, you can use the clusterSelector field in the OverrideRule spec. The clusterSelector field supports the following fields:\nclusterSelectorTerms: A list of terms that are used to select clusters. Each term in the list is used to select clusters based on the label selector. JSON Patch Override To specify the changes to be applied to the selected resources, you can use the jsonPatchOverrides field in the OverrideRule spec. The jsonPatchOverrides field supports the following fields:\nJSONPatchOverride applies a JSON patch on the selected resources following RFC 6902. All the fields defined follow this RFC.\nop: The operation to be performed. The supported operations are add, remove, and replace.\nadd: Adds a new value to the specified path. remove: Removes the value at the specified path. replace: Replaces the value at the specified path. path: The path to the field to be modified.\nSome guidelines for the path are as follows: Must start with a / character. Cannot be empty. Cannot contain an empty string (\"///\"). Cannot be a TypeMeta Field (\"/kind\", “/apiVersion”). Cannot be a Metadata Field (\"/metadata/name\", “/metadata/namespace”), except the fields “/metadata/annotations” and “metadata/labels”. Cannot be any field in the status of the resource. Some examples of valid paths are: /metadata/labels/new-label /metadata/annotations/new-annotation /spec/template/spec/containers/0/resources/limits/cpu /spec/template/spec/containers/0/resources/requests/memory value: The value to be set.\nIf the op is remove, the value cannot be set. Multiple Override Patches You may add multiple JSONPatchOverride to an OverrideRule to apply multiple changes to the selected cluster resources.\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ClusterResourceOverride metadata: name: example-cro spec: clusterResourceSelectors: - group: rbac.authorization.k8s.io kind: ClusterRole version: v1 name: secret-reader policy: overrideRules: - clusterSelector: clusterSelectorTerms: - labelSelector: matchLabels: env: prod jsonPatchOverrides: - op: remove path: /rules/0/verbs/2 - op: remove path: /rules/0/verbs/1 The ClusterResourceOverride object above will remove the verbs “list” and “watch” in the ClusterRole named secret-reader on clusters with the label env: prod.\nBreaking down the paths: First JSONPatchOverride: /rules/0: This denotes the first rule in the rules array of the ClusterRole. In the provided ClusterRole definition, there’s only one rule defined (“secrets”), so this corresponds to the first (and only) rule. /verbs/2: Within this rule, the third element of the verbs array is targeted (“list”). Second JSONPatchOverride: /rules/0: This denotes the first rule in the rules array of the ClusterRole. In the provided ClusterRole definition, there’s only one rule defined (“secrets”), so this corresponds to the first (and only) rule. /verbs/1: Within this rule, the second element of the verbs array is targeted (“watch”). The ClusterResourceOverride mentioned above utilizes the cluster role displayed below:\nName: secret-reader Labels: \u003cnone\u003e Annotations: \u003cnone\u003e PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- secrets [] [] [get watch list] Applying the ClusterResourceOverride Create a ClusterResourcePlacement resource to specify the placement rules for distributing the cluster resource overrides across the cluster infrastructure. Ensure that you select the appropriate resource.\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp-example spec: resourceSelectors: - group: rbac.authorization.k8s.io kind: ClusterRole version: v1 name: secret-reader policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: env: prod The ClusterResourcePlacement configuration outlined above will disperse resources across all clusters labeled with env: prod. As the changes are implemented, the corresponding ClusterResourceOverride configurations will be applied to the designated clusters, triggered by the selection of matching cluster role resource secret-reader.\nVerifying the Cluster Resource is Overridden To ensure that the ClusterResourceOverride object is applied to the selected clusters, verify the ClusterResourcePlacement status by running kubectl describe crp crp-example command:\nStatus: Conditions: ... Last Transition Time: 2024-04-27T04:18:00Z Message: The selected resources are successfully overridden in the 10 clusters Observed Generation: 1 Reason: OverriddenSucceeded Status: True Type: ClusterResourcePlacementOverridden ... Observed Resource Index: 0 Placement Statuses: Applicable Cluster Resource Overrides: example-cro-0 Cluster Name: member-50 Conditions: ... Message: Successfully applied the override rules on the resources Observed Generation: 1 Reason: OverriddenSucceeded Status: True Type: Overridden ... Each cluster maintains its own Applicable Cluster Resource Overrides which contain the cluster resource override snapshot if relevant. Additionally, individual status messages for each cluster indicates whether the override rules have been effectively applied.\nThe ClusterResourcePlacementOverridden condition indicates whether the resource override has been successfully applied to the selected resources in the selected clusters.\nTo verify that the ClusterResourceOverride object has been successfully applied to the selected resources, check resources in the selected clusters:\nGet cluster credentials: az aks get-credentials --resource-group \u003cresource-group\u003e --name \u003ccluster-name\u003e Get the ClusterRole object in the selected cluster: kubectl --context=\u003cmember-cluster-context\u003e get clusterrole secret-reader -o yaml Upon inspecting the described ClusterRole object, it becomes apparent that the verbs “watch” and “list” have been removed from the permissions list within the ClusterRole named “secret-reader” on the selected cluster.\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: ... rules: - apiGroups: - \"\" resources: - secrets verbs: - get ","categories":"","description":"How to use the `ClusterResourceOverride` API to override cluster-scoped resources","excerpt":"How to use the `ClusterResourceOverride` API to override …","ref":"/website/docs/how-tos/cluster-resource-override/","tags":"","title":"Using the ClusterResourceOverride API"},{"body":"Packages cluster.kubernetes-fleet.io/v1beta1 placement.kubernetes-fleet.io/v1beta1 cluster.kubernetes-fleet.io/v1beta1 Resource Types MemberCluster MemberClusterList AgentStatus AgentStatus defines the observed status of the member agent of the given type.\nAppears in:\nMemberClusterStatus Field Description type AgentType Type of the member agent. conditions Condition array Conditions is an array of current observed conditions for the member agent. lastReceivedHeartbeat Time Last time we received a heartbeat from the member agent. AgentType Underlying type: string\nAgentType defines a type of agent/binary running in a member cluster.\nAppears in:\nAgentStatus MemberCluster MemberCluster is a resource created in the hub cluster to represent a member cluster within a fleet.\nAppears in:\nMemberClusterList Field Description apiVersion string cluster.kubernetes-fleet.io/v1beta1 kind string MemberCluster kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec MemberClusterSpec The desired state of MemberCluster. status MemberClusterStatus The observed status of MemberCluster. MemberClusterList MemberClusterList contains a list of MemberCluster.\nField Description apiVersion string cluster.kubernetes-fleet.io/v1beta1 kind string MemberClusterList kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ListMeta Refer to Kubernetes API documentation for fields of metadata. items MemberCluster array MemberClusterSpec MemberClusterSpec defines the desired state of MemberCluster.\nAppears in:\nMemberCluster Field Description identity _Subject The identity used by the member cluster to access the hub cluster. The hub agents deployed on the hub cluster will automatically grant the minimal required permissions to this identity for the member agents deployed on the member cluster to access the hub cluster. heartbeatPeriodSeconds _integer How often (in seconds) for the member cluster to send a heartbeat to the hub cluster. Default: 60 seconds. Min: 1 second. Max: 10 minutes. taints array Taint An array of taints where each taint attached to the MemberCluster has that “effect” on any ClusterResourcePlacement that does not tolerate the taint. MemberClusterStatus MemberClusterStatus defines the observed status of MemberCluster.\nAppears in:\nMemberCluster Field Description conditions Condition array Conditions is an array of current observed conditions for the member cluster. resourceUsage ResourceUsage The current observed resource usage of the member cluster. It is copied from the corresponding InternalMemberCluster object. agentStatus AgentStatus array AgentStatus is an array of current observed status, each corresponding to one member agent running in the member cluster. ResourceUsage ResourceUsage contains the observed resource usage of a member cluster.\nAppears in:\nMemberClusterStatus Field Description capacity ResourceList Capacity represents the total resource capacity of all the nodes on a member cluster. allocatable ResourceList Allocatable represents the total resources of all the nodes on a member cluster that are available for scheduling. observationTime Time When the resource usage is observed. Taint A taint is a \u003ckey, value, effect\u003e triple which when attached to the MemberCluster has that “effect” on any ClusterResourcePlacement that does not tolerate the taint.\nAppears in:\nMemberClusterSpec Field Description key string The taint key to be applied to the MemberCluster. value string The taint value to be applied to the MemberCluster. effect string Effect is the effect of the taint on any ClusterResourcePlacement that does not tolerate the taint. Effect Underlying type: string\nEffect is the effect of the taint on any ClusterResourcePlacement.\nAppears in:\nTaint Toleration placement.kubernetes-fleet.io/v1beta1 Resource Types ClusterResourceBinding ClusterResourcePlacement ClusterResourceSnapshot ClusterSchedulingPolicySnapshot Work WorkList Affinity Affinity is a group of cluster affinity scheduling rules. More to be added.\nAppears in:\nPlacementPolicy Field Description clusterAffinity ClusterAffinity ClusterAffinity contains cluster affinity scheduling rules for the selected resources. BindingState Underlying type: string\nBindingState is the state of the binding.\nAppears in:\nResourceBindingSpec ClusterDecision ClusterDecision represents a decision from a placement An empty ClusterDecision indicates it is not scheduled yet.\nAppears in:\nResourceBindingSpec SchedulingPolicySnapshotStatus Field Description clusterName string ClusterName is the name of the ManagedCluster. If it is not empty, its value should be unique cross all placement decisions for the Placement. selected boolean Selected indicates if this cluster is selected by the scheduler. clusterScore ClusterScore ClusterScore represents the score of the cluster calculated by the scheduler. reason string Reason represents the reason why the cluster is selected or not. ClusterResourceBinding ClusterResourceBinding represents a scheduling decision that binds a group of resources to a cluster. It MUST have a label named CRPTrackingLabel that points to the cluster resource policy that creates it.\nAppears in:\nClusterResourceBindingList Field Description apiVersion string placement.kubernetes-fleet.io/v1beta1 kind string ClusterResourceBinding kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec ResourceBindingSpec The desired state of ClusterResourceBinding. status ResourceBindingStatus The observed status of ClusterResourceBinding. ClusterResourcePlacement ClusterResourcePlacement is used to select cluster scoped resources, including built-in resources and custom resources, and placement them onto selected member clusters in a fleet. If a namespace is selected, ALL the resources under the namespace are placed to the target clusters unless allowed-propagating-apis flag is configured on hub-agent. Note that you can’t select the following resources: - reserved namespaces including: default, kube-* (reserved for Kubernetes system namespaces), fleet-* (reserved for fleet system namespaces). - reserved fleet resource types including: MemberCluster, InternalMemberCluster, ClusterResourcePlacement, ClusterSchedulingPolicySnapshot, ClusterResourceSnapshot, ClusterResourceBinding, etc. ClusterSchedulingPolicySnapshot and ClusterResourceSnapshot objects are created when there are changes in the system to keep the history of the changes affecting a ClusterResourcePlacement.\nAppears in:\nClusterResourcePlacementList Field Description apiVersion string placement.kubernetes-fleet.io/v1beta1 kind string ClusterResourcePlacement kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec ClusterResourcePlacementSpec The desired state of ClusterResourcePlacement. status ClusterResourcePlacementStatus The observed status of ClusterResourcePlacement. ClusterResourcePlacementSpec ClusterResourcePlacementSpec defines the desired state of ClusterResourcePlacement.\nAppears in:\nClusterResourcePlacement Field Description resourceSelectors ClusterResourceSelector array ResourceSelectors is an array of selectors used to select cluster scoped resources. The selectors are ORed. You can have 1-100 selectors. policy PlacementPolicy Policy defines how to select member clusters to place the selected resources. If unspecified, all the joined member clusters are selected. strategy RolloutStrategy The rollout strategy to use to replace existing placement with new ones. revisionHistoryLimit integer The number of old ClusterSchedulingPolicySnapshot or ClusterResourceSnapshot resources to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified. Defaults to 10. ClusterResourcePlacementStatus ClusterResourcePlacementStatus defines the observed state of the ClusterResourcePlacement object.\nAppears in:\nClusterResourcePlacement Field Description selectedResources ResourceIdentifier array SelectedResources contains a list of resources selected by ResourceSelectors. observedResourceIndex string Resource index logically represents the generation of the selected resources. We take a new snapshot of the selected resources whenever the selection or their content change. Each snapshot has a different resource index. One resource snapshot can contain multiple clusterResourceSnapshots CRs in order to store large amount of resources. To get clusterResourceSnapshot of a given resource index, use the following command: kubectl get ClusterResourceSnapshot --selector=kubernetes-fleet.io/resource-index=$ObservedResourceIndex ObservedResourceIndex is the resource index that the conditions in the ClusterResourcePlacementStatus observe. For example, a condition of ClusterResourcePlacementSynchronized type is observing the synchronization status of the resource snapshot with the resource index $ObservedResourceIndex. placementStatuses ResourcePlacementStatus array PlacementStatuses contains a list of placement status on the clusters that are selected by PlacementPolicy. Each selected cluster according to the latest resource placement is guaranteed to have a corresponding placementStatuses. In the pickN case, there are N placement statuses where N = NumberOfClusters; Or in the pickFixed case, there are N placement statuses where N = ClusterNames. In these cases, some of them may not have assigned clusters when we cannot fill the required number of clusters. TODO, For pickAll type, considering providing unselected clusters info. conditions Condition array Conditions is an array of current observed conditions for ClusterResourcePlacement. ClusterResourceSelector ClusterResourceSelector is used to select cluster scoped resources as the target resources to be placed. If a namespace is selected, ALL the resources under the namespace are selected automatically. All the fields are ANDed. In other words, a resource must match all the fields to be selected.\nAppears in:\nClusterResourcePlacementSpec Field Description group string Group name of the cluster-scoped resource. Use an empty string to select resources under the core API group (e.g., namespaces). version string Version of the cluster-scoped resource. kind string Kind of the cluster-scoped resource. Note: When Kind is namespace, ALL the resources under the selected namespaces are selected. name string Name of the cluster-scoped resource. labelSelector LabelSelector A label query over all the cluster-scoped resources. Resources matching the query are selected. Note that namespace-scoped resources can’t be selected even if they match the query. ClusterResourceSnapshot ClusterResourceSnapshot is used to store a snapshot of selected resources by a resource placement policy. Its spec is immutable. We may need to produce more than one resourceSnapshot for all the resources a ResourcePlacement selected to get around the 1MB size limit of k8s objects. We assign an ever-increasing index for each such group of resourceSnapshots. The naming convention of a clusterResourceSnapshot is {CRPName}-{resourceIndex}-{subindex} where the name of the first snapshot of a group has no subindex part so its name is {CRPName}-{resourceIndex}-snapshot. resourceIndex will begin with 0. Each snapshot MUST have the following labels: - CRPTrackingLabel which points to its owner CRP. - ResourceIndexLabel which is the index of the snapshot group. - IsLatestSnapshotLabel which indicates whether the snapshot is the latest one. All the snapshots within the same index group must have the same ResourceIndexLabel. The first snapshot of the index group MUST have the following annotations: - NumberOfResourceSnapshotsAnnotation to store the total number of resource snapshots in the index group. - ResourceGroupHashAnnotation whose value is the sha-256 hash of all the snapshots belong to the same snapshot index. Each snapshot (excluding the first snapshot) MUST have the following annotations: - SubindexOfResourceSnapshotAnnotation to store the subindex of resource snapshot in the group.\nAppears in:\nClusterResourceSnapshotList Field Description apiVersion string placement.kubernetes-fleet.io/v1beta1 kind string ClusterResourceSnapshot kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec ResourceSnapshotSpec The desired state of ResourceSnapshot. status ResourceSnapshotStatus The observed status of ResourceSnapshot. ClusterSchedulingPolicySnapshot ClusterSchedulingPolicySnapshot is used to store a snapshot of cluster placement policy. Its spec is immutable. The naming convention of a ClusterSchedulingPolicySnapshot is {CRPName}-{PolicySnapshotIndex}. PolicySnapshotIndex will begin with 0. Each snapshot must have the following labels: - CRPTrackingLabel which points to its owner CRP. - PolicyIndexLabel which is the index of the policy snapshot. - IsLatestSnapshotLabel which indicates whether the snapshot is the latest one.\nAppears in:\nClusterSchedulingPolicySnapshotList Field Description apiVersion string placement.kubernetes-fleet.io/v1beta1 kind string ClusterSchedulingPolicySnapshot kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec SchedulingPolicySnapshotSpec The desired state of SchedulingPolicySnapshot. status SchedulingPolicySnapshotStatus The observed status of SchedulingPolicySnapshot. ClusterScore ClusterScore represents the score of the cluster calculated by the scheduler.\nAppears in:\nClusterDecision Field Description affinityScore integer AffinityScore represents the affinity score of the cluster calculated by the last scheduling decision based on the preferred affinity selector. An affinity score may not present if the cluster does not meet the required affinity. priorityScore integer TopologySpreadScore represents the priority score of the cluster calculated by the last scheduling decision based on the topology spread applied to the cluster. A priority score may not present if the cluster does not meet the topology spread. ClusterSelector Appears in:\nClusterAffinity Field Description clusterSelectorTerms ClusterSelectorTerm array ClusterSelectorTerms is a list of cluster selector terms. The terms are ORed. ClusterSelectorTerm ClusterSelectorTerm contains the requirements to select clusters.\nAppears in:\nClusterSelector PreferredClusterSelector Field Description labelSelector LabelSelector LabelSelector is a label query over all the joined member clusters. Clusters matching the query are selected. EnvelopeIdentifier EnvelopeIdentifier identifies the envelope object that contains the selected resource.\nAppears in:\nFailedResourcePlacement ResourceIdentifier Field Description name string Name of the envelope object. namespace string Namespace is the namespace of the envelope object. Empty if the envelope object is cluster scoped. type EnvelopeType Type of the envelope object. EnvelopeType Underlying type: string\nEnvelopeType defines the type of the envelope object.\nAppears in:\nEnvelopeIdentifier FailedResourcePlacement FailedResourcePlacement contains the failure details of a failed resource placement.\nAppears in:\nResourcePlacementStatus Field Description group string Group is the group name of the selected resource. version string Version is the version of the selected resource. kind string Kind represents the Kind of the selected resources. name string Name of the target resource. namespace string Namespace is the namespace of the resource. Empty if the resource is cluster scoped. envelope EnvelopeIdentifier Envelope identifies the envelope object that contains this resource. condition Condition The failed condition status. Manifest Manifest represents a resource to be deployed on spoke cluster.\nAppears in:\nWorkloadTemplate ManifestCondition ManifestCondition represents the conditions of the resources deployed on spoke cluster.\nAppears in:\nWorkStatus Field Description identifier WorkResourceIdentifier resourceId represents a identity of a resource linking to manifests in spec. conditions Condition array Conditions represents the conditions of this resource on spoke cluster PlacementPolicy PlacementPolicy contains the rules to select target member clusters to place the selected resources. Note that only clusters that are both joined and satisfying the rules will be selected. You can only specify at most one of the two fields: ClusterNames and Affinity. If none is specified, all the joined clusters are selected.\nAppears in:\nClusterResourcePlacementSpec SchedulingPolicySnapshotSpec Field Description placementType PlacementType Type of placement. Can be “PickAll”, “PickN” or “PickFixed”. Default is PickAll. clusterNames string array ClusterNames contains a list of names of MemberCluster to place the selected resources. Only valid if the placement type is “PickFixed” numberOfClusters integer NumberOfClusters of placement. Only valid if the placement type is “PickN”. affinity Affinity Affinity contains cluster affinity scheduling rules. Defines which member clusters to place the selected resources. Only valid if the placement type is “PickAll” or “PickN”. topologySpreadConstraints TopologySpreadConstraint array TopologySpreadConstraints describes how a group of resources ought to spread across multiple topology domains. Scheduler will schedule resources in a way which abides by the constraints. All topologySpreadConstraints are ANDed. Only valid if the placement type is “PickN”. tolerations Toleration array An array of tolerations where each toleration is used to tolerate one or more taints on the MemberCluster based on key, value, effect and operator. PlacementType Underlying type: string\nPlacementType identifies the type of placement.\nAppears in:\nPlacementPolicy PreferredClusterSelector Appears in:\nClusterAffinity Field Description weight integer Weight associated with matching the corresponding clusterSelectorTerm, in the range [-100, 100]. preference ClusterSelectorTerm A cluster selector term, associated with the corresponding weight. ResourceBindingSpec ResourceBindingSpec defines the desired state of ClusterResourceBinding.\nAppears in:\nClusterResourceBinding Field Description state BindingState The desired state of the binding. Possible values: Scheduled, Bound, Unscheduled. resourceSnapshotName string ResourceSnapshotName is the name of the resource snapshot that this resource binding points to. If the resources are divided into multiple snapshots because of the resource size limit, it points to the name of the leading snapshot of the index group. schedulingPolicySnapshotName string SchedulingPolicySnapshotName is the name of the scheduling policy snapshot that this resource binding points to; more specifically, the scheduler creates this bindings in accordance with this scheduling policy snapshot. targetCluster string TargetCluster is the name of the cluster that the scheduler assigns the resources to. clusterDecision ClusterDecision ClusterDecision explains why the scheduler selected this cluster. ResourceBindingStatus ResourceBindingStatus represents the current status of a ClusterResourceBinding.\nAppears in:\nClusterResourceBinding Field Description conditions Condition array Conditions is an array of current observed conditions for ClusterResourceBinding. ResourceContent ResourceContent contains the content of a resource\nAppears in:\nResourceSnapshotSpec ResourceIdentifier ResourceIdentifier identifies one Kubernetes resource.\nAppears in:\nClusterResourcePlacementStatus FailedResourcePlacement Field Description group string Group is the group name of the selected resource. version string Version is the version of the selected resource. kind string Kind represents the Kind of the selected resources. name string Name of the target resource. namespace string Namespace is the namespace of the resource. Empty if the resource is cluster scoped. envelope EnvelopeIdentifier Envelope identifies the envelope object that contains this resource. ResourcePlacementStatus ResourcePlacementStatus represents the placement status of selected resources for one target cluster.\nAppears in:\nClusterResourcePlacementStatus Field Description clusterName string ClusterName is the name of the cluster this resource is assigned to. If it is not empty, its value should be unique cross all placement decisions for the Placement. failedPlacements FailedResourcePlacement array FailedResourcePlacements is a list of all the resources failed to be placed to the given cluster. Note that we only include 100 failed resource placements even if there are more than 100. This field is only meaningful if the ClusterName is not empty. conditions Condition array Conditions is an array of current observed conditions for ResourcePlacementStatus. ResourceSnapshotSpec ResourceSnapshotSpec\tdefines the desired state of ResourceSnapshot.\nAppears in:\nClusterResourceSnapshot Field Description selectedResources ResourceContent array SelectedResources contains a list of resources selected by ResourceSelectors. ResourceSnapshotStatus Appears in:\nClusterResourceSnapshot Field Description conditions Condition array Conditions is an array of current observed conditions for ResourceSnapshot. RollingUpdateConfig RollingUpdateConfig contains the config to control the desired behavior of rolling update.\nAppears in:\nRolloutStrategy Field Description maxUnavailable IntOrString The maximum number of clusters that can be unavailable during the rolling update comparing to the desired number of clusters. The desired number equals to the NumberOfClusters field when the placement type is PickN. The desired number equals to the number of clusters scheduler selected when the placement type is PickAll. Value can be an absolute number (ex: 5) or a percentage of the desired number of clusters (ex: 10%). Absolute number is calculated from percentage by rounding up. We consider a resource unavailable when we either remove it from a cluster or in-place upgrade the resources content on the same cluster. This can not be 0 if MaxSurge is 0. Defaults to 25%. maxSurge IntOrString The maximum number of clusters that can be scheduled above the desired number of clusters. The desired number equals to the NumberOfClusters field when the placement type is PickN. The desired number equals to the number of clusters scheduler selected when the placement type is PickAll. Value can be an absolute number (ex: 5) or a percentage of desire (ex: 10%). Absolute number is calculated from percentage by rounding up. This does not apply to the case that we do in-place upgrade of resources on the same cluster. This can not be 0 if MaxUnavailable is 0. Defaults to 25%. unavailablePeriodSeconds integer UnavailablePeriodSeconds is used to config the time to wait between rolling out phases. A resource placement is considered available after UnavailablePeriodSeconds seconds has passed after the resources are applied to the target cluster successfully. Default is 60. RolloutStrategy RolloutStrategy describes how to roll out a new change in selected resources to target clusters.\nAppears in:\nClusterResourcePlacementSpec Field Description type RolloutStrategyType Type of rollout. The only supported type is “RollingUpdate”. Default is “RollingUpdate”. rollingUpdate RollingUpdateConfig Rolling update config params. Present only if RolloutStrategyType = RollingUpdate. RolloutStrategyType Underlying type: string\nAppears in:\nRolloutStrategy SchedulingPolicySnapshotSpec SchedulingPolicySnapshotSpec defines the desired state of SchedulingPolicySnapshot.\nAppears in:\nClusterSchedulingPolicySnapshot Field Description policy PlacementPolicy Policy defines how to select member clusters to place the selected resources. If unspecified, all the joined member clusters are selected. policyHash byte array PolicyHash is the sha-256 hash value of the Policy field. SchedulingPolicySnapshotStatus SchedulingPolicySnapshotStatus defines the observed state of SchedulingPolicySnapshot.\nAppears in:\nClusterSchedulingPolicySnapshot Field Description observedCRPGeneration integer ObservedCRPGeneration is the generation of the CRP which the scheduler uses to perform the scheduling cycle and prepare the scheduling status. conditions Condition array Conditions is an array of current observed conditions for SchedulingPolicySnapshot. targetClusters ClusterDecision array ClusterDecisions contains a list of names of member clusters considered by the scheduler. Note that all the selected clusters must present in the list while not all the member clusters are guaranteed to be listed due to the size limit. We will try to add the clusters that can provide the most insight to the list first. TopologySpreadConstraint TopologySpreadConstraint specifies how to spread resources among the given cluster topology.\nAppears in:\nPlacementPolicy Field Description maxSkew integer MaxSkew describes the degree to which resources may be unevenly distributed. When whenUnsatisfiable=DoNotSchedule, it is the maximum permitted difference between the number of resource copies in the target topology and the global minimum. The global minimum is the minimum number of resource copies in a domain. When whenUnsatisfiable=ScheduleAnyway, it is used to give higher precedence to topologies that satisfy it. It’s an optional field. Default value is 1 and 0 is not allowed. topologyKey string TopologyKey is the key of cluster labels. Clusters that have a label with this key and identical values are considered to be in the same topology. We consider each \u003ckey, value\u003e as a “bucket”, and try to put balanced number of replicas of the resource into each bucket honor the MaxSkew value. It’s a required field. whenUnsatisfiable UnsatisfiableConstraintAction WhenUnsatisfiable indicates how to deal with the resource if it doesn’t satisfy the spread constraint. - DoNotSchedule (default) tells the scheduler not to schedule it. - ScheduleAnyway tells the scheduler to schedule the resource in any cluster, but giving higher precedence to topologies that would help reduce the skew. It’s an optional field. UnsatisfiableConstraintAction Underlying type: string\nUnsatisfiableConstraintAction defines the type of actions that can be taken if a constraint is not satisfied.\nAppears in:\nTopologySpreadConstraint Toleration Toleration is used by ClusterResourcePlacement to tolerate any taint that matches the triple \u003ckey,value,effect\u003e using the matching operator .\nAppears in:\nPlacementPolicy Field Description key string The key refers to the taint key that the toleration applies to. operator string Operator represents a key’s relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. value string The value refers to the taint value that the toleration applies to. If the operator is Exists, the value should be empty. effect string Effect is the effect of the taint on any ClusterResourcePlacement that does not tolerate the taint. Work Work is the Schema for the works API.\nAppears in:\nWorkList Field Description apiVersion string placement.kubernetes-fleet.io/v1beta1 kind string Work kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec WorkSpec spec defines the workload of a work. status WorkStatus status defines the status of each applied manifest on the spoke cluster. WorkList WorkList contains a list of Work.\nField Description apiVersion string placement.kubernetes-fleet.io/v1beta1 kind string WorkList kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources metadata ListMeta Refer to Kubernetes API documentation for fields of metadata. items Work array List of works. WorkResourceIdentifier WorkResourceIdentifier provides the identifiers needed to interact with any arbitrary object. Renamed original “ResourceIdentifier” so that it won’t conflict with ResourceIdentifier defined in the clusterresourceplacement_types.go.\nAppears in:\nAppliedResourceMeta ManifestCondition Field Description ordinal integer Ordinal represents an index in manifests list, so the condition can still be linked to a manifest even thougth manifest cannot be parsed successfully. group string Group is the group of the resource. version string Version is the version of the resource. kind string Kind is the kind of the resource. resource string Resource is the resource type of the resource namespace string Namespace is the namespace of the resource, the resource is cluster scoped if the value is empty name string Name is the name of the resource WorkSpec WorkSpec defines the desired state of Work.\nAppears in:\nWork Field Description workload WorkloadTemplate Workload represents the manifest workload to be deployed on spoke cluster WorkStatus WorkStatus defines the observed state of Work.\nAppears in:\nWork Field Description conditions Condition array Conditions contains the different condition statuses for this work. Valid condition types are: 1. Applied represents workload in Work is applied successfully on the spoke cluster. 2. Progressing represents workload in Work in the trasitioning from one state to another the on the spoke cluster. 3. Available represents workload in Work exists on the spoke cluster. 4. Degraded represents the current state of workload does not match the desired state for a certain period. manifestConditions ManifestCondition array ManifestConditions represents the conditions of each resource in work deployed on spoke cluster. WorkloadTemplate WorkloadTemplate represents the manifest workload to be deployed on spoke cluster\nAppears in:\nWorkSpec Field Description manifests Manifest array Manifests represents a list of kuberenetes resources to be deployed on the spoke cluster. ","categories":"","description":"Reference for Fleet APIs","excerpt":"Reference for Fleet APIs","ref":"/website/docs/api-reference/","tags":"","title":"API Reference"},{"body":"One of the most important features of Fleet is the ability to safely rollout changes across multiple clusters. We do this by rolling out the changes in a controlled manner, ensuring that we only continue to propagate the changes to the next target clusters if the resources are successfully applied to the previous target clusters.\nOverview We automatically propagate any resource changes that are selected by a ClusterResourcePlacement from the hub cluster to the target clusters based on the placement policy defined in the ClusterResourcePlacement. In order to reduce the blast radius of such operation, we provide users a way to safely rollout the new changes so that a bad release won’t affect all the running instances all at once.\nRollout Strategy We currently only support the RollingUpdate rollout strategy. It updates the resources in the selected target clusters gradually based on the maxUnavailable and maxSurge settings.\nIn place update policy We always try to do in-place update by respecting the rollout strategy if there is no change in the placement. This is to avoid unnecessary interrupts to the running workloads when there is only resource changes. For example, if you only change the tag of the deployment in the namespace you want to place, we will do an in-place update on the deployments already placed on the targeted cluster instead of moving the existing deployments to other clusters even if the labels or properties of the current clusters are not the best to match the current placement policy.\nHow To Use RollingUpdateConfig RolloutUpdateConfig is used to control behavior of the rolling update strategy.\nMaxUnavailable and MaxSurge MaxUnavailable specifies the maximum number of connected clusters to the fleet compared to target number of clusters specified in ClusterResourcePlacement policy in which resources propagated by the ClusterResourcePlacement can be unavailable. Minimum value for MaxUnavailable is set to 1 to avoid stuck rollout during in-place resource update.\nMaxSurge specifies the maximum number of clusters that can be scheduled with resources above the target number of clusters specified in ClusterResourcePlacement policy.\nNote: MaxSurge only applies to rollouts to newly scheduled clusters, and doesn’t apply to rollouts of workload triggered by updates to already propagated resource. For updates to already propagated resources, we always try to do the updates in place with no surge.\ntarget number of clusters changes based on the ClusterResourcePlacement policy.\nFor PickAll, it’s the number of clusters picked by the scheduler. For PickN, it’s the number of clusters specified in the ClusterResourcePlacement policy. For PickFixed, it’s the length of the list of cluster names specified in the ClusterResourcePlacement policy. Example 1: Consider a fleet with 4 connected member clusters (cluster-1, cluster-2, cluster-3 \u0026 cluster-4) where every member cluster has label env: prod. The hub cluster has a namespace called test-ns with a deployment in it.\nThe ClusterResourcePlacement spec is defined as follows:\nspec: resourceSelectors: - group: \"\" kind: Namespace version: v1 name: test-ns policy: placementType: PickN numberOfClusters: 3 affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: env: prod strategy: rollingUpdate: maxUnavailable: 1 maxSurge: 1 The rollout will be as follows:\nWe try to pick 3 clusters out of 4, for this scenario let’s say we pick cluster-1, cluster-2 \u0026 cluster-3.\nSince we can’t track the initial availability for the deployment, we rollout the namespace with deployment to cluster-1, cluster-2 \u0026 cluster-3.\nThen we update the deployment with a bad image name to update the resource in place on cluster-1, cluster-2 \u0026 cluster-3.\nBut since we have maxUnavailable set to 1, we will rollout the bad image name update for deployment to one of the clusters (which cluster the resource is rolled out to first is non-deterministic).\nOnce the deployment is updated on the first cluster, we will wait for the deployment’s availability to be true before rolling out to the other clusters\nAnd since we rolled out a bad image name update for the deployment it’s availability will always be false and hence the rollout for the other two clusters will be stuck\nUsers might think maxSurge of 1 might be utilized here but in this case since we are updating the resource in place maxSurge will not be utilized to surge and pick cluster-4.\nNote: maxSurge will be utilized to pick cluster-4, if we change the policy to pick 4 cluster or change placement type to PickAll.\nExample 2: Consider a fleet with 4 connected member clusters (cluster-1, cluster-2, cluster-3 \u0026 cluster-4) where,\ncluster-1 and cluster-2 has label loc: west cluster-3 and cluster-4 has label loc: east The hub cluster has a namespace called test-ns with a deployment in it.\nInitially, the ClusterResourcePlacement spec is defined as follows:\nspec: resourceSelectors: - group: \"\" kind: Namespace version: v1 name: test-ns policy: placementType: PickN numberOfClusters: 2 affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: loc: west strategy: rollingUpdate: maxSurge: 2 The rollout will be as follows:\nWe try to pick clusters (cluster-1 and cluster-2) by specifying the label selector loc: west. Since we can’t track the initial availability for the deployment, we rollout the namespace with deployment to cluster-1 and cluster-2 and wait till they become available. Then we update the ClusterResourcePlacement spec to the following:\nspec: resourceSelectors: - group: \"\" kind: Namespace version: v1 name: test-ns policy: placementType: PickN numberOfClusters: 2 affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: loc: east strategy: rollingUpdate: maxSurge: 2 The rollout will be as follows:\nWe try to pick clusters (cluster-3 and cluster-4) by specifying the label selector loc: east. But this time around since we have maxSurge set to 2 we are saying we can propagate resources to a maximum of 4 clusters but our target number of clusters specified is 2, we will rollout the namespace with deployment to both cluster-3 and cluster-4 before removing the deployment from cluster-1 and cluster-2. And since maxUnavailable is always set to 25% by default which is rounded off to 1, we will remove the resource from one of the existing clusters (cluster-1 or cluster-2) because when maxUnavailable is 1 the policy mandates at least one cluster to be available. UnavailablePeriodSeconds UnavailablePeriodSeconds is used to configure the waiting time between rollout phases when we cannot determine if the resources have rolled out successfully or not. This field is used only if the availability of resources we propagate are not trackable. Refer to the Data only object section for more details.\nAvailability based Rollout We have built-in mechanisms to determine the availability of some common Kubernetes native resources. We only mark them as available in the target clusters when they meet the criteria we defined.\nHow It Works We have an agent running in the target cluster to check the status of the resources. We have specific criteria for each of the following resources to determine if they are available or not. Here are the list of resources we support:\nDeployment We only mark a Deployment as available when all its pods are running, ready and updated according to the latest spec.\nDaemonSet We only mark a DaemonSet as available when all its pods are available and updated according to the latest spec on all desired scheduled nodes.\nStatefulSet We only mark a StatefulSet as available when all its pods are running, ready and updated according to the latest revision.\nJob We only mark a Job as available when it has at least one succeeded pod or one ready pod.\nService For Service based on the service type the availability is determined as follows:\nFor ClusterIP \u0026 NodePort service, we mark it as available when a cluster IP is assigned. For LoadBalancer service, we mark it as available when a LoadBalancerIngress has been assigned along with an IP or Hostname. For ExternalName service, checking availability is not supported, so it will be marked as available with not trackable reason. Data only objects For the objects described below since they are a data resource we mark them as available immediately after creation,\nNamespace Secret ConfigMap Role ClusterRole RoleBinding ClusterRoleBinding ","categories":"","description":"Concept about rolling out changes safely in Fleet","excerpt":"Concept about rolling out changes safely in Fleet","ref":"/website/docs/concepts/safe-rollout/","tags":"","title":"Safe Rollout"},{"body":"This guide provides an overview of how to use the Fleet ResourceOverride API to override resources.\nOverview ResourceOverride is a Fleet API that allows you to modify or override specific attributes of existing resources within your cluster. With ResourceOverride, you can define rules based on cluster labels or other criteria, specifying changes to be applied to resources such as Deployments, StatefulSets, ConfigMaps, or Secrets. These changes can include updates to container images, environment variables, resource limits, or any other configurable parameters.\nAPI Components The ResourceOverride API consists of the following components:\nResource Selectors: These specify the set of resources selected for overriding. Policy: This specifies the policy to be applied to the selected resources. The following sections discuss these components in depth.\nResource Selectors A ResourceOverride object may feature one or more resource selectors, specifying which resources to select to be overridden.\nThe ResourceSelector object supports the following fields:\ngroup: The API group of the resource version: The API version of the resource kind: The kind of the resource name: The name of the resource Note: The resource can only be selected by name.\nTo add a resource selector, edit the resourceSelectors field in the ResourceOverride spec:\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ResourceOverride metadata: name: example-ro namespace: test-namespace spec: resourceSelectors: - group: apps kind: Deployment version: v1 name: my-deployment Note: The ResourceOverride needs to be in the same namespace as the resources it is overriding.\nThe example above will pick a Deployment named my-deployment from the namespace test-namespace, as shown below, to be overridden.\napiVersion: apps/v1 kind: Deployment metadata: ... name: my-deployment namespace: test-namespace ... spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: test-nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: test-nginx spec: containers: - image: nginx:1.14.2 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: ... Policy The Policy is made up of a set of rules (OverrideRules) that specify the changes to be applied to the selected resources on selected clusters.\nEach OverrideRule supports the following fields:\nCluster Selector: This specifies the set of clusters to which the override applies. JSON Patch Override: This specifies the changes to be applied to the selected resources. To add an override rule, edit the policy field in the ResourceOverride spec:\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ResourceOverride metadata: name: example-ro namespace: test-namespace spec: resourceSelectors: - group: apps kind: Deployment version: v1 name: my-deployment policy: overrideRules: - clusterSelector: clusterSelectorTerms: - labelSelector: matchLabels: env: prod jsonPatchOverrides: - op: replace path: /spec/template/spec/containers/0/image value: \"nginx:1.20.0\" The ResourceOverride object above will replace the image of the container in the Deployment named my-deployment with the image nginx:1.20.0 on all clusters with the label env: prod.\nThe ResourceOverride mentioned above utilizes the deployment displayed below:\napiVersion: apps/v1 kind: Deployment metadata: ... name: my-deployment namespace: test-namespace ... spec: ... template: ... spec: containers: - image: nginx:1.14.2 imagePullPolicy: IfNotPresent name: nginx ports: ... ... ... Cluster Selector To specify the clusters to which the override applies, you can use the clusterSelector field in the OverrideRule spec. The clusterSelector field supports the following fields:\nclusterSelectorTerms: A list of terms that are used to select clusters. Each term in the list is used to select clusters based on the label selector. JSON Patch Override To specify the changes to be applied to the selected resources, you can use the jsonPatchOverrides field in the OverrideRule spec. The jsonPatchOverrides field supports the following fields:\nJSONPatchOverride applies a JSON patch on the selected resources following RFC 6902. All the fields defined follow this RFC.\nThe jsonPatchOverrides field supports the following fields:\nop: The operation to be performed. The supported operations are add, remove, and replace.\nadd: Adds a new value to the specified path. remove: Removes the value at the specified path. replace: Replaces the value at the specified path. path: The path to the field to be modified.\nSome guidelines for the path are as follows: Must start with a / character. Cannot be empty. Cannot contain an empty string (\"///\"). Cannot be a TypeMeta Field (\"/kind\", “/apiVersion”). Cannot be a Metadata Field (\"/metadata/name\", “/metadata/namespace”), except the fields “/metadata/annotations” and “metadata/labels”. Cannot be any field in the status of the resource. Some examples of valid paths are: /metadata/labels/new-label /metadata/annotations/new-annotation /spec/template/spec/containers/0/resources/limits/cpu /spec/template/spec/containers/0/resources/requests/memory value: The value to be set.\nIf the op is remove, the value cannot be set. Multiple Override Rules You may add multiple OverrideRules to a Policy to apply multiple changes to the selected resources.\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ResourceOverride metadata: name: example-ro namespace: test-namespace spec: resourceSelectors: - group: apps kind: Deployment version: v1 name: my-deployment policy: overrideRules: - clusterSelector: clusterSelectorTerms: - labelSelector: matchLabels: env: prod jsonPatchOverrides: - op: replace path: /spec/template/spec/containers/0/image value: \"nginx:1.20.0\" - clusterSelector: clusterSelectorTerms: - labelSelector: matchLabels: env: test jsonPatchOverrides: - op: replace path: /spec/template/spec/containers/0/image value: \"nginx:latest\" The ResourceOverride object above will replace the image of the container in the Deployment named my-deployment with the image nginx:1.20.0 on all clusters with the label env: prod and the image nginx:latest on all clusters with the label env: test.\nThe ResourceOverride mentioned above utilizes the deployment displayed below:\napiVersion: apps/v1 kind: Deployment metadata: ... name: my-deployment namespace: test-namespace ... spec: ... template: ... spec: containers: - image: nginx:1.14.2 imagePullPolicy: IfNotPresent name: nginx ports: ... ... ... Applying the ResourceOverride Create a ClusterResourcePlacement resource to specify the placement rules for distributing the resource overrides across the cluster infrastructure. Ensure that you select the appropriate namespaces containing the matching resources.\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: crp-example spec: resourceSelectors: - group: \"\" kind: Namespace name: test-namespace version: v1 policy: placementType: PickAll affinity: clusterAffinity: requiredDuringSchedulingIgnoredDuringExecution: clusterSelectorTerms: - labelSelector: matchLabels: env: prod - labelSelector: matchLabels: env: test The ClusterResourcePlacement configuration outlined above will disperse resources within test-namespace across all clusters labeled with env: prod and env: test. As the changes are implemented, the corresponding ResourceOverride configurations will be applied to the designated clusters, triggered by the selection of matching deployment resource my-deployment.\nVerifying the Cluster Resource is Overridden To ensure that the ResourceOverride object is applied to the selected resources, verify the ClusterResourcePlacement status by running kubectl describe crp crp-example command:\nStatus: Conditions: ... Message: The selected resources are successfully overridden in the 10 clusters Observed Generation: 1 Reason: OverriddenSucceeded Status: True Type: ClusterResourcePlacementOverridden ... Observed Resource Index: 0 Placement Statuses: Applicable Resource Overrides: Name: example-ro-0 Namespace: test-namespace Cluster Name: member-50 Conditions: ... Last Transition Time: 2024-04-26T22:57:14Z Message: Successfully applied the override rules on the resources Observed Generation: 1 Reason: OverriddenSucceeded Status: True Type: Overridden ... Each cluster maintains its own Applicable Resource Overrides which contain the resource override snapshot and the resource override namespace if relevant. Additionally, individual status messages for each cluster indicates whether the override rules have been effectively applied.\nThe ClusterResourcePlacementOverridden condition indicates whether the resource override has been successfully applied to the selected resources in the selected clusters.\nTo verify that the ResourceOverride object has been successfully applied to the selected resources, check resources in the selected clusters:\nGet cluster credentials: az aks get-credentials --resource-group \u003cresource-group\u003e --name \u003ccluster-name\u003e Get the Deployment object in the selected cluster: kubectl --context=\u003cmember-cluster-context\u003e get deployment my-deployment -n test-namespace -o yaml Upon inspecting the member cluster, it was found that the selected cluster had the label env: prod. Consequently, the image on deployment my-deployment was modified to be nginx:1.20.0 on selected cluster.\napiVersion: apps/v1 kind: Deployment metadata: ... name: my-deployment namespace: test-namespace ... spec: ... template: ... spec: containers: - image: nginx:1.20.0 imagePullPolicy: IfNotPresent name: nginx ports: ... ... status: ... ","categories":"","description":"How to use the `ResourceOverride` API to override namespace-scoped resources","excerpt":"How to use the `ResourceOverride` API to override namespace-scoped …","ref":"/website/docs/how-tos/resource-override/","tags":"","title":"Using the ResourceOverride API"},{"body":"Overview The ClusterResourceOverride and ResourceOverride provides a way to customize resource configurations before they are propagated to the target cluster by the ClusterResourcePlacement.\nDifference Between ClusterResourceOverride And ResourceOverride ClusterResourceOverride represents the cluster-wide policy that overrides the cluster scoped resources to one or more clusters while ResourceOverride will apply to resources in the same namespace as the namespace-wide policy.\nNote: If a namespace is selected by the ClusterResourceOverride, ALL the resources under the namespace are selected automatically.\nIf the resource is selected by both ClusterResourceOverride and ResourceOverride, the ResourceOverride will win when resolving the conflicts.\nWhen To Use Override Overrides is useful when you want to customize the resources before they are propagated from the hub cluster to the target clusters. Some example use cases are:\nAs a platform operator, I want to propagate a clusterRoleBinding to cluster-us-east and cluster-us-west and would like to grant the same role to different groups in each cluster. As a platform operator, I want to propagate a clusterRole to cluster-staging and cluster-production and would like to grant more permissions to the cluster-staging cluster than the cluster-production cluster. As a platform operator, I want to propagate a namespace to all the clusters and would like to customize the labels for each cluster. As an application developer, I would like to propagate a deployment to cluster-staging and cluster-production and would like to always use the latest image in the staging cluster and a specific image in the production cluster. As an application developer, I would like to propagate a deployment to all the clusters and would like to use different commands for my container in different regions. Limits Each resource can be only selected by one override simultaneously. In the case of namespace scoped resources, up to two overrides will be allowed, considering the potential selection through both ClusterResourceOverride (select its namespace) and ResourceOverride. At most 100 ClusterResourceOverride can be created. At most 100 ResourceOverride can be created. Resource Selector ClusterResourceSelector of ClusterResourceOverride selects which cluster-scoped resources need to be overridden before applying to the selected clusters.\nIt supports the following forms of resource selection:\nSelect resources by specifying the \u003cgroup, version, kind\u003e and name. This selection propagates only one resource that matches the \u003cgroup, version, kind\u003e and name. Note: Label selector of ClusterResourceSelector is not supported.\nResourceSelector of ResourceOverride selects which namespace-scoped resources need to be overridden before applying to the selected clusters.\nIt supports the following forms of resource selection:\nSelect resources by specifying the \u003cgroup, version, kind\u003e and name. This selection propagates only one resource that matches the \u003cgroup, version, kind\u003e and name under the ResourceOverride namespace. Override Policy Override policy defines how to override the selected resources on the target clusters.\nIt contains an array of override rules and its order determines the override order. For example, when there are two rules selecting the same fields on the target cluster, the last one will win.\nEach override rule contains the following fields:\nClusterSelector: which cluster(s) the override rule applies to. It supports the following forms of cluster selection: Select clusters by specifying the cluster labels. An empty selector selects ALL the clusters. A nil selector selects NO target cluster. JSONPatchOverrides: a list of JSON path override rules applied to the selected resources following RFC 6902. Note: Updating the fields in the TypeMeta (e.g., apiVersion, kind) is not allowed.\nNote: Updating the fields in the ObjectMeta (e.g., name, namespace) excluding annotations and labels is not allowed.\nNote: Updating the fields in the Status (e.g., status) is not allowed.\nWhen To Trigger Rollout It will take the snapshot of each override change as a result of ClusterResourceOverrideSnapshot and ResourceOverrideSnapshot. The snapshot will be used to determine whether the override change should be applied to the existing ClusterResourcePlacement or not. If applicable, it will start rolling out the new resources to the target clusters by respecting the rollout strategy defined in the ClusterResourcePlacement.\nExamples add annotations to the configmap by using clusterResourceOverride Suppose we create a configmap named app-config-1 under the namespace application-1 in the hub cluster, and we want to add an annotation to it, which is applied to all the member clusters.\napiVersion: v1 data: data: test kind: ConfigMap metadata: creationTimestamp: \"2024-05-07T08:06:27Z\" name: app-config-1 namespace: application-1 resourceVersion: \"1434\" uid: b4109de8-32f2-4ac8-9e1a-9cb715b3261d Create a ClusterResourceOverride named cro-1 to add an annotation to the namespace application-1.\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ClusterResourceOverride metadata: creationTimestamp: \"2024-05-07T08:06:27Z\" finalizers: - kubernetes-fleet.io/override-cleanup generation: 1 name: cro-1 resourceVersion: \"1436\" uid: 32237804-7eb2-4d5f-9996-ff4d8ce778e7 spec: clusterResourceSelectors: - group: \"\" kind: Namespace name: application-1 version: v1 policy: overrideRules: - clusterSelector: clusterSelectorTerms: [] jsonPatchOverrides: - op: add path: /metadata/annotations value: cro-test-annotation: cro-test-annotation-val Check the configmap on one of the member cluster by running kubectl get configmap app-config-1 -n application-1 -o yaml command:\napiVersion: v1 data: data: test kind: ConfigMap metadata: annotations: cro-test-annotation: cro-test-annotation-val kubernetes-fleet.io/last-applied-configuration: '{\"apiVersion\":\"v1\",\"data\":{\"data\":\"test\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{\"cro-test-annotation\":\"cro-test-annotation-val\",\"kubernetes-fleet.io/spec-hash\":\"4dd5a08aed74884de455b03d3b9c48be8278a61841f3b219eca9ed5e8a0af472\"},\"name\":\"app-config-1\",\"namespace\":\"application-1\",\"ownerReferences\":[{\"apiVersion\":\"placement.kubernetes-fleet.io/v1beta1\",\"blockOwnerDeletion\":false,\"kind\":\"AppliedWork\",\"name\":\"crp-1-work\",\"uid\":\"77d804f5-f2f1-440e-8d7e-e9abddacb80c\"}]}}' kubernetes-fleet.io/spec-hash: 4dd5a08aed74884de455b03d3b9c48be8278a61841f3b219eca9ed5e8a0af472 creationTimestamp: \"2024-05-07T08:06:27Z\" name: app-config-1 namespace: application-1 ownerReferences: - apiVersion: placement.kubernetes-fleet.io/v1beta1 blockOwnerDeletion: false kind: AppliedWork name: crp-1-work uid: 77d804f5-f2f1-440e-8d7e-e9abddacb80c resourceVersion: \"1449\" uid: a8601007-1e6b-4b64-bc05-1057ea6bd21b add annotations to the configmap by using resourceOverride You can use the ResourceOverride to add an annotation to the configmap app-config-1 explicitly in the namespace application-1.\napiVersion: placement.kubernetes-fleet.io/v1alpha1 kind: ResourceOverride metadata: creationTimestamp: \"2024-05-07T08:25:31Z\" finalizers: - kubernetes-fleet.io/override-cleanup generation: 1 name: ro-1 namespace: application-1 resourceVersion: \"3859\" uid: b4117925-bc3c-438d-a4f6-067bc4577364 spec: policy: overrideRules: - clusterSelector: clusterSelectorTerms: [] jsonPatchOverrides: - op: add path: /metadata/annotations value: ro-test-annotation: ro-test-annotation-val resourceSelectors: - group: \"\" kind: ConfigMap name: app-config-1 version: v1 How To Validate If Overrides Are Applied You can validate if the overrides are applied by checking the ClusterResourcePlacement status. The status output will indicate both placement conditions and individual placement statuses on each member cluster that was overridden.\nSample output:\nstatus: conditions: - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 1 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: All 3 cluster(s) start rolling out the latest resource observedGeneration: 1 reason: RolloutStarted status: \"True\" type: ClusterResourcePlacementRolloutStarted - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: The selected resources are successfully overridden in the 3 clusters observedGeneration: 1 reason: OverriddenSucceeded status: \"True\" type: ClusterResourcePlacementOverridden - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: Works(s) are succcesfully created or updated in the 3 target clusters' namespaces observedGeneration: 1 reason: WorkSynchronized status: \"True\" type: ClusterResourcePlacementWorkSynchronized - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: The selected resources are successfully applied to 3 clusters observedGeneration: 1 reason: ApplySucceeded status: \"True\" type: ClusterResourcePlacementApplied - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: The selected resources in 3 cluster are available now observedGeneration: 1 reason: ResourceAvailable status: \"True\" type: ClusterResourcePlacementAvailable observedResourceIndex: \"0\" placementStatuses: - applicableClusterResourceOverrides: - cro-1-0 clusterName: kind-cluster-1 conditions: - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: 'Successfully scheduled resources for placement in kind-cluster-1 (affinity score: 0, topology spread score: 0): picked by scheduling policy' observedGeneration: 1 reason: Scheduled status: \"True\" type: Scheduled - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: Detected the new changes on the resources and started the rollout process observedGeneration: 1 reason: RolloutStarted status: \"True\" type: RolloutStarted - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: Successfully applied the override rules on the resources observedGeneration: 1 reason: OverriddenSucceeded status: \"True\" type: Overridden - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: All of the works are synchronized to the latest observedGeneration: 1 reason: AllWorkSynced status: \"True\" type: WorkSynchronized - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: All corresponding work objects are applied observedGeneration: 1 reason: AllWorkHaveBeenApplied status: \"True\" type: Applied - lastTransitionTime: \"2024-05-07T08:06:27Z\" message: The availability of work object crp-1-work is not trackable observedGeneration: 1 reason: WorkNotTrackable status: \"True\" type: Available ... applicableClusterResourceOverrides in placementStatuses indicates which ClusterResourceOverrideSnapshot that is applied to the target cluster. Similarly, applicableResourceOverrides will be set if the ResourceOverrideSnapshot is applied.\n","categories":"","description":"Concept about the override APIs","excerpt":"Concept about the override APIs","ref":"/website/docs/concepts/override/","tags":"","title":"Override"},{"body":"Propagating Resources with Envelope Objects This guide provides instructions on propagating a set of resources from the hub cluster to joined member clusters within an envelope object.\nEnvelope Object with ConfigMap Currently, we support using a ConfigMap as an envelope object by leveraging a fleet-reserved annotation.\nTo designate a ConfigMap as an envelope object, ensure that it contains the following annotation:\nmetadata: annotations: kubernetes-fleet.io/envelope-configmap: \"true\" Example ConfigMap Envelope Object: apiVersion: v1 kind: ConfigMap metadata: name: envelope-configmap namespace: app annotations: kubernetes-fleet.io/envelope-configmap: \"true\" data: resourceQuota.yaml: | apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: app spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi webhook.yaml: | apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: creationTimestamp: null labels: azure-workload-identity.io/system: \"true\" name: azure-wi-webhook-mutating-webhook-configuration webhooks: - admissionReviewVersions: - v1 - v1beta1 clientConfig: service: name: azure-wi-webhook-webhook-service namespace: app path: /mutate-v1-pod failurePolicy: Fail matchPolicy: Equivalent name: mutation.azure-workload-identity.io rules: - apiGroups: - \"\" apiVersions: - v1 operations: - CREATE - UPDATE resources: - pods sideEffects: None Propagating an Envelope ConfigMap from Hub cluster to Member cluster: We will now apply the example envelope object above on our hub cluster. Then we use a ClusterResourcePlacement object to propagate the resource from hub to a member cluster named kind-cluster-1.\nCRP spec: spec: policy: clusterNames: - kind-cluster-1 placementType: PickFixed resourceSelectors: - group: \"\" kind: Namespace name: app version: v1 revisionHistoryLimit: 10 strategy: type: RollingUpdate CRP status: status: conditions: - lastTransitionTime: \"2023-11-30T19:54:13Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 2 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2023-11-30T19:54:18Z\" message: All 1 cluster(s) are synchronized to the latest resources on the hub cluster observedGeneration: 2 reason: SynchronizeSucceeded status: \"True\" type: ClusterResourcePlacementSynchronized - lastTransitionTime: \"2023-11-30T19:54:18Z\" message: Successfully applied resources to 1 member clusters observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: ClusterResourcePlacementApplied placementStatuses: - clusterName: kind-cluster-1 conditions: - lastTransitionTime: \"2023-11-30T19:54:13Z\" message: 'Successfully scheduled resources for placement in kind-cluster-1: picked by scheduling policy' observedGeneration: 2 reason: ScheduleSucceeded status: \"True\" type: ResourceScheduled - lastTransitionTime: \"2023-11-30T19:54:18Z\" message: Successfully Synchronized work(s) for placement observedGeneration: 2 reason: WorkSynchronizeSucceeded status: \"True\" type: WorkSynchronized - lastTransitionTime: \"2023-11-30T19:54:18Z\" message: Successfully applied resources observedGeneration: 2 reason: ApplySucceeded status: \"True\" type: ResourceApplied selectedResources: - kind: Namespace name: app version: v1 - kind: ConfigMap name: envelope-configmap namespace: app version: v1 Note: In the selectedResources section, we specifically display the propagated envelope object. Please note that we do not individually list all the resources contained within the envelope object in the status.\nUpon inspection of the selectedResources, it indicates that the namespace app and the configmap envelope-configmap have been successfully propagated. Users can further verify the successful propagation of resources mentioned within the envelope-configmap object by ensuring that the failedPlacements section in the placementStatus for kind-cluster-1 does not appear in the status.\nExample CRP status where resource within an envelope object failed to apply: CRP status: In the example below, within the placementStatus section for kind-cluster-1, the failedPlacements section provides details on resource that failed to apply along with information about the envelope object which contained the resource.\nstatus: conditions: - lastTransitionTime: \"2023-12-06T00:09:53Z\" message: found all the clusters needed as specified by the scheduling policy observedGeneration: 2 reason: SchedulingPolicyFulfilled status: \"True\" type: ClusterResourcePlacementScheduled - lastTransitionTime: \"2023-12-06T00:09:58Z\" message: All 1 cluster(s) are synchronized to the latest resources on the hub cluster observedGeneration: 2 reason: SynchronizeSucceeded status: \"True\" type: ClusterResourcePlacementSynchronized - lastTransitionTime: \"2023-12-06T00:09:58Z\" message: Failed to apply manifests to 1 clusters, please check the `failedPlacements` status observedGeneration: 2 reason: ApplyFailed status: \"False\" type: ClusterResourcePlacementApplied placementStatuses: - clusterName: kind-cluster-1 conditions: - lastTransitionTime: \"2023-12-06T00:09:53Z\" message: 'Successfully scheduled resources for placement in kind-cluster-1: picked by scheduling policy' observedGeneration: 2 reason: ScheduleSucceeded status: \"True\" type: ResourceScheduled - lastTransitionTime: \"2023-12-06T00:09:58Z\" message: Successfully Synchronized work(s) for placement observedGeneration: 2 reason: WorkSynchronizeSucceeded status: \"True\" type: WorkSynchronized - lastTransitionTime: \"2023-12-06T00:09:58Z\" message: Failed to apply manifests, please check the `failedPlacements` status observedGeneration: 2 reason: ApplyFailed status: \"False\" type: ResourceApplied failedPlacements: - condition: lastTransitionTime: \"2023-12-06T00:09:53Z\" message: 'Failed to apply manifest: namespaces \"app\" not found' reason: AppliedManifestFailedReason status: \"False\" type: Applied envelope: name: envelop-configmap namespace: test-ns type: ConfigMap kind: ResourceQuota name: mem-cpu-demo namespace: app version: v1 selectedResources: - kind: Namespace name: test-ns version: v1 - kind: ConfigMap name: envelop-configmap namespace: test-ns version: v1 ","categories":"","description":"How to use envelope objects with the ClusterResourcePlacement API","excerpt":"How to use envelope objects with the ClusterResourcePlacement API","ref":"/website/docs/how-tos/envelope-object/","tags":"","title":"Using Envelope Objects to Place Resources"},{"body":"While users rely on the RollingUpdate rollout strategy to safely roll out their workloads, there is also a requirement for a staged rollout mechanism at the cluster level to enable more controlled and systematic continuous delivery (CD) across the fleet. Introducing a staged update run feature would address this need by enabling gradual deployments, reducing risk, and ensuring greater reliability and consistency in workload updates across clusters.\nOverview We introduce two new Custom Resources, ClusterStagedUpdateStrategy and ClusterStagedUpdateRun.\nClusterStagedUpdateStrategy defines a reusable orchestration pattern that organizes member clusters into distinct stages, controlling both the rollout sequence within each stage and incorporating post-stage validation tasks that must succeed before proceeding to subsequent stages. For brevity, we’ll refer to ClusterStagedUpdateStrategy as updateRun strategy throughout this document.\nClusterStagedUpdateRun orchestrates resource deployment across clusters by executing a ClusterStagedUpdateStrategy. It requires three key inputs: the target ClusterResourcePlacement name, a resource snapshot index specifying the version to deploy, and the strategy name that defines the rollout rules. The term updateRun will be used to represent ClusterStagedUpdateRun in this document.\nSpecify Rollout Strategy for ClusterResourcePlacement While ClusterResourcePlacement uses RollingUpdate as its default strategy, switching to staged updates requires setting the rollout strategy to External:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: example-placement spec: resourceSelectors: - group: \"\" kind: Namespace name: test-namespace version: v1 policy: placementType: PickAll tolerations: - key: gpu-workload operator: Exists strategy: type: External # specify External here to use the stagedUpdateRun strategy. Deploy a ClusterStagedUpdateStrategy The ClusterStagedUpdateStrategy custom resource enables users to organize member clusters into stages and define their rollout sequence. This strategy is reusable across multiple updateRuns, with each updateRun creating an immutable snapshot of the strategy at startup. This ensures that modifications to the strategy do not impact any in-progress updateRun executions.\nAn example ClusterStagedUpdateStrategy looks like below:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterStagedUpdateStrategy metadata: name: example-strategy spec: stages: - name: staging labelSelector: matchLabels: environment: staging afterStageTasks: - type: TimedWait waitTime: 1h - name: canary labelSelector: matchLabels: environment: canary afterStageTasks: - type: Approval - name: production labelSelector: matchLabels: environment: production sortingLabelKey: order afterStageTasks: - type: Approval - type: TimedWait waitTime: 1h ClusterStagedUpdateStrategy is cluster-scoped resource. Its spec contains a list of stageConfig entries defining the configuration for each stage. Stages execute sequentially in the order specified. Each stage must have a unique name and uses a labelSelector to identify member clusters for update. In above example, we define 3 stages: staging selecting member clusters labeled with environment: staging, canary selecting member clusters labeled with environment: canary and production selecting member clusters labeled with environment: production.\nEach stage can optionally specify sortingLabelKey and afterStageTasks. sortingLabelKey is used to define a label whose integer value determines update sequence within a stage. With above example, assuming there are 3 clusters selected in the production (all 3 clusters have environment: production label), then the fleet admin can label them with order: 1, order: 2, and order: 3 respectively to control the rollout sequence. Without sortingLabelKey, clusters are updated in alphabetical order by name.\nBy default, the next stage begins immediately after the current stage completes. A user can control this cross-stage behavior by specifying the afterStageTasks in each stage. These tasks execute after all clusters in a stage update successfully. We currently support two types of tasks: Approval and Timedwait. Each stage can include one task of each type (maximum of two tasks). Both tasks must be satisfied before advancing to the next stage.\nTimedwait task requires a specified waitTime duration. The updateRun waits for the duration to pass before executing the next stage. For Approval task, the controller generates a ClusterApprovalRequest object automatically named as \u003cupdateRun name\u003e-\u003cstage name\u003e. The name is also shown in the updateRun status. The ClusterApprovalRequest object is pretty simple:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterApprovalRequest metadata: name: example-run-canary labels: kubernetes-fleet.io/targetupdaterun: example-run kubernetes-fleet.io/targetUpdatingStage: canary kubernetes-fleet.io/isLatestUpdateRunApproval: \"true\" spec: parentStageRollout: example-run targetStage: canary The user then need to manually approve the task by patching its status:\nkubectl patch clusterapprovalrequests example-run-canary --type='merge' -p '{\"status\":{\"conditions\":[{\"type\":\"Approved\",\"status\":\"True\",\"reason\":\"lgtm\",\"message\":\"lgtm\",\"lastTransitionTime\":\"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\"observedGeneration\":1}]}}' --subresource=status The updateRun will only continue to next stage after the ClusterApprovalRequest is approved.\nTrigger rollout with ClusterStagedUpdateRun When using External rollout strategy, a ClusterResourcePlacement begins deployment only when triggered by a ClusterStagedUpdateRun. An example ClusterStagedUpdateRun is shown below:\napiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterStagedUpdateRun metadata: name: example-run spec: placementName: example-placement resourceSnapshotIndex: \"0\" stagedRolloutStrategyName: example-strategy This cluster-scoped resource requires three key parameters: the placementName specifying the target ClusterResourcePlacement, the resourceSnapshotIndex identifying which version of resources to deploy (learn how to find resourceSnapshotIndex here), and the stagedRolloutStrategyName indicating the ClusterStagedUpdateStrategy to follow.\nAn updateRun executes in two phases. During the initialization phase, the controller performs a one-time setup where it captures a snapshot of the updateRun strategy, collects scheduled and to-be-deleted ClusterResourceBindings, generates the cluster update sequence, and records all this information in the updateRun status.\nIn the execution phase, the controller processes each stage sequentially, updates clusters within each stage one at a time, and enforces completion of after-stage tasks. It then executes a final delete stage to clean up resources from unscheduled clusters. The updateRun succeeds when all stages complete successfully. However, it will fail if any execution-affecting events occur, for example, the target ClusterResourcePlacement being deleted, and member cluster changes triggering new scheduling. In such cases, error details are recorded in the updateRun status. Remember that once initialized, an updateRun operates on its strategy snapshot, making it immune to subsequent strategy modifications.\nUnderstand ClusterStagedUpdateRun status Let’s take a deep look into the status of a completed ClusterStagedUpdateRun. It displays details about the rollout status for every clusters and stages.\n$ kubectl describe crsur run example-run ... Status: Conditions: Last Transition Time: 2025-03-12T23:21:39Z Message: ClusterStagedUpdateRun initialized successfully Observed Generation: 1 Reason: UpdateRunInitializedSuccessfully Status: True Type: Initialized Last Transition Time: 2025-03-12T23:21:39Z Message: Observed Generation: 1 Reason: UpdateRunStarted Status: True Type: Progressing Last Transition Time: 2025-03-12T23:26:15Z Message: Observed Generation: 1 Reason: UpdateRunSucceeded Status: True Type: Succeeded Deletion Stage Status: Clusters: Conditions: Last Transition Time: 2025-03-12T23:26:15Z Message: Observed Generation: 1 Reason: StageUpdatingStarted Status: True Type: Progressing Last Transition Time: 2025-03-12T23:26:15Z Message: Observed Generation: 1 Reason: StageUpdatingSucceeded Status: True Type: Succeeded End Time: 2025-03-12T23:26:15Z Stage Name: kubernetes-fleet.io/deleteStage Start Time: 2025-03-12T23:26:15Z Policy Observed Cluster Count: 2 Policy Snapshot Index Used: 0 Staged Update Strategy Snapshot: Stages: After Stage Tasks: Type: Approval Wait Time: 0s Type: TimedWait Wait Time: 1m0s Label Selector: Match Labels: Environment: staging Name: staging After Stage Tasks: Type: Approval Wait Time: 0s Label Selector: Match Labels: Environment: canary Name: canary Sorting Label Key: name After Stage Tasks: Type: TimedWait Wait Time: 1m0s Type: Approval Wait Time: 0s Label Selector: Match Labels: Environment: production Name: production Sorting Label Key: order Stages Status: After Stage Task Status: Approval Request Name: example-run-staging Conditions: Last Transition Time: 2025-03-12T23:21:54Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestCreated Status: True Type: ApprovalRequestCreated Last Transition Time: 2025-03-12T23:22:55Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestApproved Status: True Type: ApprovalRequestApproved Type: Approval Conditions: Last Transition Time: 2025-03-12T23:22:54Z Message: Observed Generation: 1 Reason: AfterStageTaskWaitTimeElapsed Status: True Type: WaitTimeElapsed Type: TimedWait Clusters: Cluster Name: member1 Conditions: Last Transition Time: 2025-03-12T23:21:39Z Message: Observed Generation: 1 Reason: ClusterUpdatingStarted Status: True Type: Started Last Transition Time: 2025-03-12T23:21:54Z Message: Observed Generation: 1 Reason: ClusterUpdatingSucceeded Status: True Type: Succeeded Conditions: Last Transition Time: 2025-03-12T23:21:54Z Message: Observed Generation: 1 Reason: StageUpdatingWaiting Status: False Type: Progressing Last Transition Time: 2025-03-12T23:22:55Z Message: Observed Generation: 1 Reason: StageUpdatingSucceeded Status: True Type: Succeeded End Time: 2025-03-12T23:22:55Z Stage Name: staging Start Time: 2025-03-12T23:21:39Z After Stage Task Status: Approval Request Name: example-run-canary Conditions: Last Transition Time: 2025-03-12T23:23:10Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestCreated Status: True Type: ApprovalRequestCreated Last Transition Time: 2025-03-12T23:25:15Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestApproved Status: True Type: ApprovalRequestApproved Type: Approval Clusters: Cluster Name: member2 Conditions: Last Transition Time: 2025-03-12T23:22:55Z Message: Observed Generation: 1 Reason: ClusterUpdatingStarted Status: True Type: Started Last Transition Time: 2025-03-12T23:23:10Z Message: Observed Generation: 1 Reason: ClusterUpdatingSucceeded Status: True Type: Succeeded Conditions: Last Transition Time: 2025-03-12T23:23:10Z Message: Observed Generation: 1 Reason: StageUpdatingWaiting Status: False Type: Progressing Last Transition Time: 2025-03-12T23:25:15Z Message: Observed Generation: 1 Reason: StageUpdatingSucceeded Status: True Type: Succeeded End Time: 2025-03-12T23:25:15Z Stage Name: canary Start Time: 2025-03-12T23:22:55Z After Stage Task Status: Conditions: Last Transition Time: 2025-03-12T23:26:15Z Message: Observed Generation: 1 Reason: AfterStageTaskWaitTimeElapsed Status: True Type: WaitTimeElapsed Type: TimedWait Approval Request Name: example-run-production Conditions: Last Transition Time: 2025-03-12T23:25:15Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestCreated Status: True Type: ApprovalRequestCreated Last Transition Time: 2025-03-12T23:25:25Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestApproved Status: True Type: ApprovalRequestApproved Type: Approval Clusters: Conditions: Last Transition Time: 2025-03-12T23:25:15Z Message: Observed Generation: 1 Reason: StageUpdatingWaiting Status: False Type: Progressing Last Transition Time: 2025-03-12T23:26:15Z Message: Observed Generation: 1 Reason: StageUpdatingSucceeded Status: True Type: Succeeded End Time: 2025-03-12T23:26:15Z Stage Name: production Events: \u003cnone\u003e UpdateRun overall status At the very top, Status.Conditions gives the overall status of the updateRun. The execution an update run consists of two phases: initialization and execution. During initialization, the controller performs a one-time setup where it captures a snapshot of the updateRun strategy, collects scheduled and to-be-deleted ClusterResourceBindings, generates the cluster update sequence, and records all this information in the updateRun status. The UpdateRunInitializedSuccessfully condition indicates the initialization is successful.\nAfter initialization, the controller starts executing the updateRun. The UpdateRunStarted condition indicates the execution has started.\nAfter all clusters are updated, all after-stage tasks are completed, and thus all stages are finished, the UpdateRunSucceeded condition is set to True, indicating the updateRun has succeeded.\nFields recorded in the updateRun status during initialization During initialization, the controller records the following fields in the updateRun status:\nPolicySnapshotIndexUsed: the index of the policy snapshot used for the updateRun, it should be the latest one. PolicyObservedClusterCount: the number of clusters selected by the scheduling policy. StagedUpdateStrategySnapshot: the snapshot of the updateRun strategy, which ensures any strategy changes will not affect executing updateRuns. Stages and clusters status The Stages Status section displays the status of each stage and cluster. As shown in the strategy snapshot, the updateRun has three stages: staging, canary, and production. During initialization, the controller generates the rollout plan, classifies the scheduled clusters into these three stages and dumps the plan into the updateRun status. As the execution progresses, the controller updates the status of each stage and cluster. Take the staging stage as an example, member1 is included in this stage. ClusterUpdatingStarted condition indicates the cluster is being updated and ClusterUpdatingSucceeded condition shows the cluster is updated successfully.\nAfter all clusters are updated in a stage, the controller executes the specified after-stage tasks. Stage staging has two after-stage tasks: Approval and TimedWait. The Approval task requires the admin to manually approve a ClusterApprovalRequest generated by the controller. The name of the ClusterApprovalRequest is also included in the status, which is example-run-staging. AfterStageTaskApprovalRequestCreated condition indicates the approval request is created and AfterStageTaskApprovalRequestApproved condition indicates the approval request has been approved. The TimedWait task enforces a suspension of the rollout until the specified wait time has elapsed and in this case, the wait time is 1 minute. AfterStageTaskWaitTimeElapsed condition indicates the wait time has elapsed and the rollout can proceed to the next stage.\nEach stage also has its own conditions. When a stage starts, the Progressing condition is set to True. When all the cluster updates complete, the Progressing condition is set to False with reason StageUpdatingWaiting as shown above. It means the stage is waiting for after-stage tasks to pass. And thus the lastTransitionTime of the Progressing condition also serves as the start time of the wait in case there’s a TimedWait task. When all after-stage tasks pass, the Succeeded condition is set to True. Each stage status also has Start Time and End Time fields, making it easier to read.\nThere’s also a Deletion Stage Status section, which displays the status of the deletion stage. The deletion stage is the last stage of the updateRun. It deletes resources from the unscheduled clusters. The status is pretty much the same as a normal update stage, except that there are no after-stage tasks.\nNote that all these conditions have lastTransitionTime set to the time when the controller updates the status. It can help debug and check the progress of the updateRun.\nRelationship between ClusterStagedUpdateRun and ClusterResourcePlacement A ClusterStagedUpdateRun serves as the trigger mechanism for rolling out a ClusterResourcePlacement. The key points of this relationship are:\nThe ClusterResourcePlacement remains in a scheduled state without being deployed until a corresponding ClusterStagedUpdateRun is created. During rollout, the ClusterResourcePlacement status is continuously updated with detailed information from each target cluster. While a ClusterStagedUpdateRun only indicates whether updates have started and completed for each member cluster (as described in previous section), the ClusterResourcePlacement provides comprehensive details including: Success/failure of resource creation Application of overrides Specific error messages For example, below is the status of an in-progress ClusterStagedUpdateRun:\nkubectl describe crsur example-run Name: example-run ... Status: Conditions: Last Transition Time: 2025-03-17T21:37:14Z Message: ClusterStagedUpdateRun initialized successfully Observed Generation: 1 Reason: UpdateRunInitializedSuccessfully Status: True Type: Initialized Last Transition Time: 2025-03-17T21:37:14Z Message: Observed Generation: 1 Reason: UpdateRunStarted # updateRun started Status: True Type: Progressing ... Stages Status: After Stage Task Status: Approval Request Name: example-run-staging Conditions: Last Transition Time: 2025-03-17T21:37:29Z Message: Observed Generation: 1 Reason: AfterStageTaskApprovalRequestCreated Status: True Type: ApprovalRequestCreated Type: Approval Conditions: Last Transition Time: 2025-03-17T21:38:29Z Message: Observed Generation: 1 Reason: AfterStageTaskWaitTimeElapsed Status: True Type: WaitTimeElapsed Type: TimedWait Clusters: Cluster Name: member1 Conditions: Last Transition Time: 2025-03-17T21:37:14Z Message: Observed Generation: 1 Reason: ClusterUpdatingStarted Status: True Type: Started Last Transition Time: 2025-03-17T21:37:29Z Message: Observed Generation: 1 Reason: ClusterUpdatingSucceeded # member1 has updated successfully Status: True Type: Succeeded Conditions: Last Transition Time: 2025-03-17T21:37:29Z Message: Observed Generation: 1 Reason: StageUpdatingWaiting # waiting for approval Status: False Type: Progressing Stage Name: staging Start Time: 2025-03-17T21:37:14Z After Stage Task Status: Approval Request Name: example-run-canary Type: Approval Clusters: Cluster Name: member2 Stage Name: canary After Stage Task Status: Type: TimedWait Approval Request Name: example-run-production Type: Approval Clusters: Stage Name: production ... In above status, member1 from stage staging has been updated successfully. The stage is waiting for approval to proceed to the next stage. And member2 from stage canary is not updated yet.\nLet’s take a look at the status of the ClusterResourcePlacement example-placement:\nkubectl describe crp example-placement Name: example-placement ... Status: Conditions: Last Transition Time: 2025-03-12T23:01:32Z Message: found all cluster needed as specified by the scheduling policy, found 2 cluster(s) Observed Generation: 1 Reason: SchedulingPolicyFulfilled Status: True Type: ClusterResourcePlacementScheduled Last Transition Time: 2025-03-13T07:35:25Z Message: There are still 1 cluster(s) in the process of deciding whether to roll out the latest resources or not Observed Generation: 1 Reason: RolloutStartedUnknown Status: Unknown Type: ClusterResourcePlacementRolloutStarted Observed Resource Index: 5 Placement Statuses: Cluster Name: member1 Conditions: Last Transition Time: 2025-03-12T23:01:32Z Message: Successfully scheduled resources for placement in \"member1\" (affinity score: 0, topology spread score: 0): picked by scheduling policy Observed Generation: 1 Reason: Scheduled Status: True Type: Scheduled Last Transition Time: 2025-03-17T21:37:14Z Message: Detected the new changes on the resources and started the rollout process, resourceSnapshotIndex: 5, clusterStagedUpdateRun: example-run Observed Generation: 1 Reason: RolloutStarted Status: True Type: RolloutStarted Last Transition Time: 2025-03-17T21:37:14Z Message: No override rules are configured for the selected resources Observed Generation: 1 Reason: NoOverrideSpecified Status: True Type: Overridden Last Transition Time: 2025-03-17T21:37:14Z Message: All of the works are synchronized to the latest Observed Generation: 1 Reason: AllWorkSynced Status: True Type: WorkSynchronized Last Transition Time: 2025-03-17T21:37:14Z Message: All corresponding work objects are applied Observed Generation: 1 Reason: AllWorkHaveBeenApplied Status: True Type: Applied Last Transition Time: 2025-03-17T21:37:14Z Message: All corresponding work objects are available Observed Generation: 1 Reason: AllWorkAreAvailable # member1 is all good Status: True Type: Available Cluster Name: member2 Conditions: Last Transition Time: 2025-03-12T23:01:32Z Message: Successfully scheduled resources for placement in \"member2\" (affinity score: 0, topology spread score: 0): picked by scheduling policy Observed Generation: 1 Reason: Scheduled Status: True Type: Scheduled Last Transition Time: 2025-03-13T07:35:25Z Message: In the process of deciding whether to roll out the latest resources or not Observed Generation: 1 Reason: RolloutStartedUnknown # member2 is not updated yet Status: Unknown Type: RolloutStarted ... In the Placement Statuses section, we can see the status of each member cluster. For member1, the RolloutStarted condition is set to True, indicating the rollout has started. In the condition message, we print the ClusterStagedUpdateRun name, which is example-run. This indicates the most recent cluster update is triggered by example-run. It also displays the detailed update status: the works are synced and applied and are detected available. As a comparison, member2 is still in Scheduled state only.\nWhen troubleshooting a stalled updateRun, examining the ClusterResourcePlacement status offers valuable diagnostic information that can help identify the root cause. For comprehensive troubleshooting steps, refer to the troubleshooting guide.\nConcurrent updateRuns Multiple concurrent ClusterStagedUpdateRuns can be created for the same ClusterResourcePlacement, allowing fleet administrators to pipeline the rollout of different resource versions. However, to maintain consistency across the fleet and prevent member clusters from running different resource versions simultaneously, we enforce a key constraint: all concurrent ClusterStagedUpdateRuns must use identical ClusterStagedUpdateStrategy settings.\nThis strategy consistency requirement is validated during the initialization phase of each updateRun. This validation ensures predictable rollout behavior and prevents configuration drift across your cluster fleet, even when multiple updates are in progress.\nNext Steps Learn how to rollout and rollback CRP resources with Staged Update Run Learn how to troubleshoot a Staged Update Run ","categories":"","description":"Concept about Staged Update","excerpt":"Concept about Staged Update","ref":"/website/docs/concepts/staged-update/","tags":"","title":"Staged Update"},{"body":"This document explains the concept of Eviction and Placement Disruption Budget in the context of the fleet.\nOverview Eviction provides a way to force remove resources from a target cluster once the resources have already been propagated from the hub cluster by a Placement object. Eviction is considered as an voluntary disruption triggered by the user. Eviction alone doesn’t guarantee that resources won’t be propagated to target cluster again by the scheduler. The users need to use taints in conjunction with Eviction to prevent the scheduler from picking the target cluster again.\nThe Placement Disruption Budget object protects against voluntary disruptions.\nThe only voluntary disruption that can occur in the fleet is the eviction of resources from a target cluster which can be achieved by creating the ClusterResourcePlacementEviction object.\nSome cases of involuntary disruptions in the context of fleet,\nThe removal of resources from a member cluster by the scheduler due to scheduling policy changes. Users manually deleting workload resources running on a member cluster. Users manually deleting the ClusterResourceBinding object which is an internal resource the represents the placement of resources on a member cluster. Workloads failing to run properly on a member cluster due to misconfiguration or cluster related issues. For all the cases of involuntary disruptions described above, the Placement Disruption Budget object does not protect against them.\nClusterResourcePlacementEviction An eviction object is used to remove resources from a member cluster once the resources have already been propagated from the hub cluster.\nThe eviction object is only reconciled once after which it reaches a terminal state. Below is the list of terminal states for ClusterResourcePlacementEviction,\nClusterResourcePlacementEviction is valid and it’s executed successfully. ClusterResourcePlacementEviction is invalid. ClusterResourcePlacementEviction is valid but it’s not executed. To successfully evict resources from a cluster, the user needs to specify:\nThe name of the ClusterResourcePlacement object which propagated resources to the target cluster. The name of the target cluster from which we need to evict resources. When specifying the ClusterResourcePlacement object in the eviction’s spec, the user needs to consider the following cases:\nFor PickFixed CRP, eviction is not allowed; it is recommended that one directly edit the list of target clusters on the CRP object. For PickAll \u0026 PickN CRPs, eviction is allowed because the users cannot deterministically pick or unpick a cluster based on the placement strategy; it’s up to the scheduler. Note: After an eviction is executed, there is no guarantee that the cluster won’t be picked again by the scheduler to propagate resources for a ClusterResourcePlacement resource. The user needs to specify a taint on the cluster to prevent the scheduler from picking the cluster again. This is especially true for PickAll ClusterResourcePlacement because the scheduler will try to propagate resources to all the clusters in the fleet.\nClusterResourcePlacementDisruptionBudget The ClusterResourcePlacementDisruptionBudget is used to protect resources propagated by a ClusterResourcePlacement to a target cluster from voluntary disruption, i.e., ClusterResourcePlacementEviction.\nNote: When specifying a ClusterResourcePlacementDisruptionBudget, the name should be the same as the ClusterResourcePlacement that it’s trying to protect.\nUsers are allowed to specify one of two fields in the ClusterResourcePlacementDisruptionBudget spec since they are mutually exclusive:\nMaxUnavailable - specifies the maximum number of clusters in which a placement can be unavailable due to any form of disruptions. MinAvailable - specifies the minimum number of clusters in which placements are available despite any form of disruptions. for both MaxUnavailable and MinAvailable, the user can specify the number of clusters as an integer or as a percentage of the total number of clusters in the fleet.\nNote: For both MaxUnavailable and MinAvailable, involuntary disruptions are not subject to the disruption budget but will still count against it.\nWhen specifying a disruption budget for a particular ClusterResourcePlacement, the user needs to consider the following cases:\nCRP type MinAvailable DB with an integer MinAvailable DB with a percentage MaxUnavailable DB with an integer MaxUnavailable DB with a percentage PickFixed ❌ ❌ ❌ ❌ PickAll ✅ ❌ ❌ ❌ PickN ✅ ✅ ✅ ✅ Note: We don’t allow eviction for PickFixed CRP and hence specifying a ClusterResourcePlacementDisruptionBudget for PickFixed CRP does nothing. And for PickAll CRP, the user can only specify MinAvailable because total number of clusters selected by a PickAll CRP is non-deterministic. If the user creates an invalid ClusterResourcePlacementDisruptionBudget object, when an eviction is created, the eviction won’t be successfully executed.\n","categories":"","description":"Concept about Eviction and Placement Disrupiton Budget","excerpt":"Concept about Eviction and Placement Disrupiton Budget","ref":"/website/docs/concepts/eviction-pdb/","tags":"","title":"Eviction and Placement Disruption Budget"},{"body":"Welcome ✨ This documentation can help you learn more about the KubeFleet project, get started with a KubeFleet deployment of your own, and complete common KubeFleet related tasks.\nWIP We are actively working on the documentation site. About KubeFleet KubeFleet is a CNCF sandbox project that aims to simplify Kubernetes multi-cluster management . It can greatly enhance your multi-cluster management experience; specifically, with the help of KubeFleet, one is able to easily:\nmanage clusters through one unified portal; and place Kubernetes resources across a group of clusters with advanced scheduling capabilities; and roll out changes progressively; and perform administrative tasks easily, such as observing application status, detecting configuration drifts, migrating workloads across clusters, etc. Is KubeFleet right for my multi-cluster setup? ✅ KubeFleet can work with any Kubernetes clusters running supported Kubernetes versions, regardless of where they are set up.\nYou can set up KubeFleet with an on-premises cluster, a cluster hosted on public clouds such as Azure, or even a local kind cluster.\n✅ KubeFleet can manage Kubernetes cluster groups of various sizes.\nKubeFleet is designed with performance and scalablity in mind. It functions well with both smaller Kubernetes cluster groups and those with up to hundreds of Kubernetes clusters and thousands of nodes.\n🚀 KubeFleet is evolving fast.\nWe are actively developing new features and functionalities for KubeFleet. If you have any questions, suggestions, or feedbacks, please let us know.\nGet started Find out how to deploy KubeFleet with one of our Getting Started tutorials. You can use a local setup to experiment with KubeFleet’s features, and explore its UX.\n","categories":"","description":"","excerpt":"Welcome ✨ This documentation can help you learn more about the …","ref":"/website/docs/","tags":"","title":"Welcome to KubeFleet Documentation"},{"body":"欢迎✨ 我们还在编辑KubeFleet的中文技术文档；在中文文档上线之前，请参阅KubeFleet英文技术文档来了解更多有关KubeFleet的内容。感谢您的耐心等候🙏\n","categories":"","description":"","excerpt":"欢迎✨ 我们还在编辑KubeFleet的中文技术文档；在中文文档上线之前，请参阅KubeFleet英文技术文档来了解更多有 …","ref":"/website/zh-cn/docs/","tags":"","title":"Fleet Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/website/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/website/zh-cn/categories/","tags":"","title":"Categories"},{"body":" Managing Kubernetes Clusters Seamlessly. At scale. KubeFleet is a CNCF sandbox project for easily managing your Kubernetes multi-cluster setup.\nDocumentation Source Code KubeFleet is a powerful open-source platform for multi-cluster management. Use Fleet to schedule workloads smartly, roll out changes progressively, and perform administrative tasks easily, across a group of Kubernetes clusters on any cloud or on-premises. Centralized management KubeFleet enables developers and admins to use one Kubernetes cluster, known as the hub cluster, as the portal for running all multi-cluster tasks.\nSmart scheduling Leverage KubeFleet’s flexible scheduling capabilities to always run workloads in the most appropriate clusters, maximizing performance, availiability and cost efficiency.\nSimplified administration Use KubeFleet to perform administrative tasks at ease. KubeFleet can help you enforce policies, observe status, detect configuration drifts, move workloads around as needed, and do many more.\nWe are a Cloud Native Computing Foundation sandbox project. We have sessions featuring KubeFleet at KubeCon + CloudNativeCon Europe 2025 (Apr 1-4, 2025)! Learn more about the sessions here. Questions and Discussions Join the conversation on GitHub to find solutions, submit ideas \u0026 feedbacks, and learn more about KubeFleet.\nBugs and Issues Submit a ticket if you find any bugs or are having problems using KubeFleet.\nContributions Open a pull request to contribute to the KubeFleet project.\n","categories":"","description":"","excerpt":" Managing Kubernetes Clusters Seamlessly. At scale. KubeFleet is a …","ref":"/website/","tags":"","title":"KubeFleet"},{"body":" 轻松管理多Kubernetes集群 简洁 快速 高度可伸缩 KubeFleet是一个帮助您高效管理多Kubernetes集群的CNCF沙箱项目项目。\n技术文档 源代码 KubeFleet是一个强大的开源Kubernetes多集群管理平台。 KubeFleet可以帮助您智慧调度工作负载，渐进式应用代码更新，以及轻松执行各种多集群日常管理任务。 不论您的集群是运行在公有云还是私有云上，KubeFleet都可以发挥作用。 统一集中管理 通过设置一个主集群，KubeFleet可以帮助开发者和运维人员在一个位置统一集中执行多集群操作。\n智慧调度 您可以利用KubeFleet的智慧调度能力，使您的工作负载始终在最合适的Kubernetes集群上运行，从而提升性能、改善可用性，并减少开销。\n简化管理 KubeFleet可以帮助您执行各种日常管理操作。您可以利用KubeFleet应用各类策略，检查应用状态，防止意外或临时操作造成的配置漂移，甚至是在不同集群间迁移工作负载。\nKubeFleet是一个云原生计算基金会（CNCF）旗下的沙箱项目。 如果您准备参加四月一日至四日的KubeCon + CloudNativeCon Europe 2025，您可以在这些活动中了解更多有关KubeFleet的信息。 问题与讨论 在GitHub上参与讨论。您可以就您在使用KubeFleet的过程中遇到的困惑向社区寻求帮助，或者向开发者们提出您的想法和建议。\n遇到了Bugs？ 如果您发现了一个KubeFleet的bug，或是碰到了任何使用问题，请向我们提交一份问题报告单。\n贡献代码 您可以在GitHub上发起一个Pull request，向KubeFleet项目贡献代码。\n","categories":"","description":"","excerpt":" 轻松管理多Kubernetes集群 简洁 快速 高度可伸缩 KubeFleet是一个帮助您高效管理多Kubernetes集群的CNCF沙箱 …","ref":"/website/zh-cn/","tags":"","title":"KubeFleet"},{"body":"","categories":"","description":"","excerpt":"","ref":"/website/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/website/zh-cn/tags/","tags":"","title":"Tags"}]